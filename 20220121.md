# 20220121





- url/robots.txt 대상 웹 페이지 조건 확인

- User-agent 다음 규칙이 적용되는 로봇의 이름

- Disallow 차단할 url경로

- Allow 차단 된 상위 디렉토리의 하위 디렉토리에 있는 url경로이며 차단 해제할 디렉토리

- 연결확인

  - import requests

    res=requests.get("url")

    res.raise_for_status()

    res.text

- 페이지 받아오기

  - from urllib.request from urlopen

    url ="url"

    html=urlopen(url)

    html.read()

- Error 처리

  - from urllib.request import urlopen

    from urllib.request import HTTPError

    from urllib.request import URLError

    try:

    ​	html=urlopen("url")

    except HTTPError as e:

    ​	print(e)

    except URLError:

    ​	pirnt("")

    else:

    ​	print("")

- 파싱

  - import bs4

    html_str="\<html>\<div>hello\</div>\</html>"

    bs_obj=bs4.BeautifulSoup(html_str, "html.parser")

  - .find() ()찾기

  - .text 글자만 가져오기

  - .findAll() 모두 가져오기

- \("ul", {"class":"value"}> ul태그 class속성 value