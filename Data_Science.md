📈 Statistics/Math

- 고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?

    -고유값은 선형변환(혹은 행렬)이 고유벡터 방향으로 얼마나 늘어나거나 줄어드는지를 나타내는 수입니다. 고유값은 λ (람다)로 표기되며, 선형변환의 특성을 나타내는 값입니다.

    - 고유벡터는 선형변환 후에도 방향이 변하지 않는 벡터를 의미합니다. 즉, 선형변환에 의해 크기만 변하고 방향은 유지되는 벡터입니다. 고유벡터는 고유값에 대응되며, 해당 고유값에 대한 선형변환의 효과를 나타내는 벡터입니다.

    - 이 개념이 중요한 이유

        - 선형변환의 특성 분석: 고유값과 고유벡터는 선형변환의 특성을 분석하는 데 도움을 줍니다. 고유값은 변환에 의해 벡터의 크기가 어떻게 변하는지를 알려주고, 고유벡터는 변환에 의해 벡터의 방향이 유지되는 특성을 나타냅니다.

        - 차원 축소: 고유값과 고유벡터는 데이터의 차원 축소에 활용될 수 있습니다. 주어진 데이터를 고유벡터로 투영함으로써 중요한 정보를 보존하면서 데이터의 차원을 줄일 수 있습니다.

        - 행렬 분해: 고유값과 고유벡터는 행렬 분해에 널리 사용됩니다. 고유분해(eigen decomposition)를 통해 대칭행렬을 대각화하거나, 특이분해(singular value decomposition)를 통해 임의의 행렬을 분해할 수 있습니다. 이를 통해 행렬의 특성을 이해하고, 문제를 더 간단하게 해결할 수 있습니다.


- 샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?

    - 샘플링(Sampling)은 통계학과 머신러닝에서 데이터 집합에서 일부를 추출하는 과정을 말합니다. 이를 통해 전체 데이터를 대표하는 일부 데이터를 선택할 수 있습니다. 일반적으로, 훈련 데이터를 생성하거나 모델의 성능을 평가하기 위해 샘플링을 사용합니다.

    - 리샘플링(Resampling)은 샘플링의 한 형태로, 주어진 데이터에서 반복적으로 샘플을 추출하는 과정을 말합니다. 리샘플링은 일반적으로 데이터 분석과 모델 평가에서 사용되며, 두 가지 주요 기법으로 알려져 있습니다

        - 부트스트래핑(Bootstrap): 부트스트래핑은 원래 데이터 세트에서 중복을 허용하여 샘플을 추출하는 방법입니다. 즉, 원본 데이터에서 여러 번의 샘플링을 수행하여 새로운 샘플 집합을 생성합니다. 이를 통해 샘플링 분포를 추정하거나, 추정값의 신뢰구간을 계산하는 데 사용됩니다.

        - 교차 검증(Cross-Validation): 교차 검증은 데이터를 훈련 세트와 테스트 세트로 분할하여 모델을 평가하는 방법입니다. 리샘플링 기법 중 가장 흔하게 사용되는 방법 중 하나입니다. 주로 K-겹 교차 검증(K-fold cross-validation)이 사용되며, 데이터를 K개의 서로 다른 부분 집합으로 분할한 후, 각 부분 집합을 한 번씩 테스트 세트로 사용하고 나머지 부분 집합으로 모델을 훈련합니다. 이를 K번 반복하여 모델의 성능을 평가합니다.

    - 리샘플링의 장점은 다음과 같습니다.

        - 데이터 활용: 리샘플링을 통해 주어진 데이터에서 여러 번의 샘플링을 수행함으로써, 원래 데이터셋의 크기를 효과적으로 증가시킬 수 있습니다. 이를 통해 훈련 데이터의 다양성을 높일 수 있습니다.

        - 모델 평가: 교차 검증을 통해 모델의 일반화 성능을 평가할 수 있습니다. 모델이 훈련 데이터에만 잘 맞지 않고 새로운 데이터에도 잘 적용되는지 확인할 수 있습니다.

        - 신뢰구간 추정: 부트스트래핑을 사용하여 추정값의 신뢰구간을 계산할 수 있습니다. 이를 통해 모델이나 추정값에 대한 불확실성을 추정할 수 있습니다.

    - 리샘플링은 데이터의 활용과 모델의 성능 평가에 도움을 주는 중요한 기법입니다. 이를 통해 데이터 분석과 모델 개발 과정에서 신뢰도를 높일 수 있습니다.


- 확률 모형과 확률 변수는 무엇일까요?

    - 확률 모형은 현실 세계에서 발생하는 사건들의 확률적 구조를 나타내는 수학적인 모델입니다. 이 모델은 확률 변수와 확률 분포로 구성됩니다. 확률 모형은 확률 분포를 통해 사건들이 어떻게 발생하는지를 설명하며, 이를 통해 우리가 관심을 가지는 사건들의 확률을 계산할 수 있습니다.

    - 확률 변수는 확률 모형에서 관찰 가능한 사건을 대표하는 변수입니다. 확률 변수는 특정한 값 또는 상태를 가질 수 있으며, 확률 분포에 따라 이러한 값들이 발생합니다. 예를 들어, 동전 던지기의 경우 앞면과 뒷면을 나타내는 확률 변수가 있을 수 있습니다.

    - 확률 변수는 이산적인 경우와 연속적인 경우로 나눌 수 있습니다. 이산 확률 변수는 유한한 개수 또는 셀 수 있는 개수의 값들을 가지며, 주로 이산 확률 분포를 따릅니다. 예를 들어, 주사위 던지기의 경우 1부터 6까지의 값을 가지는 이산 확률 변수가 될 수 있습니다.

    - 반면, 연속 확률 변수는 실수 범위 내에서 어떤 값을 가질 수 있으며, 주로 연속 확률 분포를 따릅니다. 예를 들어, 키나 체중과 같은 연속적인 값을 가지는 변수는 연속 확률 변수로 모델링될 수 있습니다.

    - 확률 모형과 확률 변수는 확률론의 기반이 되는 개념으로, 현실 세계에서 발생하는 사건들을 수학적으로 모델링하고 분석하는 데 사용됩니다. 이를 통해 불확실성을 다루고, 사건들의 확률을 계산하고 예측할 수 있습니다.


- 누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요.

    -누적 분포 함수(Cumulative Distribution Function, CDF)와 확률 밀도 함수(Probability Density Function, PDF)는 확률 변수의 확률 분포를 나타내는 함수입니다.

    - 누적 분포 함수(CDF)는 확률 변수가 어떤 특정 값보다 작거나 같을 확률을 나타냅니다.

        - CDF(x) = P(X ≤ x)

        - 누적 분포 함수는 확률 변수 X가 특정한 값 x보다 작거나 같을 확률을 나타내는데, 이는 확률 변수가 어떤 구간에 속할 확률을 계산하는 데에도 활용될 수 있습니다.

    - 확률 밀도 함수(PDF)는 연속 확률 변수에 대해 사용됩니다. 확률 밀도 함수는 확률 변수의 밀도를 나타내는 함수로, 특정 값 주변에서 확률 밀도를 계산할 수 있습니다. 확률 밀도 함수(PDF)는 누적 분포 함수(CDF)를 x에 대해 미분한 값으로 나타낼 수 있습니다.

        - PDF(x) = d/dx CDF(x)

        - 여기서 d/dx는 미분을 나타냅니다. 확률 밀도 함수는 누적 분포 함수의 도함수로 정의되며, 확률 변수가 특정 값에 속할 확률 밀도를 제공합니다.

    - 요약하자면, 누적 분포 함수는 확률 변수가 특정 값보다 작거나 같을 확률을 나타내는 함수이고, 확률 밀도 함수는 연속 확률 변수의 밀도를 나타내는 함수입니다. 이 두 함수를 통해 확률 변수의 확률 분포를 모델링하고, 특정 값 또는 구간에 대한 확률을 계산할 수 있습니다.


- 조건부 확률은 무엇일까요?

    - 조건부 확률은 사건 A가 주어졌을 때, 사건 B가 발생할 확률을 나타냅니다. P(B|A)로 표기되며, P(A ∩ B) / P(A)로 계산됩니다.

    - 조건부 확률은 A의 발생 여부에 따라 B의 가능성이 어떻게 변하는지를 나타냅니다. A가 주어질 때, B의 확률을 계산하여 두 사건 사이의 관계를 파악할 수 있습니다.

    - 조건부 확률은 일상적으로 다양한 분야에서 활용됩니다. 예를 들어, 날씨와 우산을 예로 들 수 있습니다. 만약 비가 올 확률이 30%라고 할 때, "오늘 날씨가 비일 때 우산을 가져갈 확률"을 조건부 확률로 계산할 수 있습니다.

    - 조건부 확률은 통계, 머신러닝, 인공지능 등 다양한 분야에서 중요한 개념입니다. 분류 모델의 예측, 추론, 상황 판단 등에 활용됩니다. 데이터를 기반으로 사건 간의 관계를 추론하고 예측하는 데에 필수적인 도구입니다.

    - 조건부 확률을 계산할 때, 사건 A와 B의 발생 확률 또는 주어진 데이터를 이용하여 계산할 수 있습니다. 일반적으로 P(A ∩ B)를 구하고, P(A)로 나누어서 조건부 확률을 계산합니다.


- 공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요.

    - 공분산과 상관계수는 두 변수 사이의 관계를 측정하는 통계적 개념입니다.

    - 공분산은 두 변수 사이의 상관성을 나타내는 값으로, 한 변수의 값이 다른 변수의 값과 얼마나 함께 변하는지를 측정합니다. 수식으로 표현하면 다음과 같습니다.

        - Cov(X, Y) = E[(X - μX)(Y - μY)]

        - μ는 "뮤"라고 읽으며, 일반적으로 평균을 나타내는 기호입니다. Cov(X, Y)는 변수 X와 Y의 공분산을 나타내며, E는 기대값(평균)을 의미합니다. μX는 변수 X의 평균, μY는 변수 Y의 평균입니다.

        - 공분산의 값이 양수인 경우, 두 변수가 양의 상관관계를 가지고 있음을 나타내며, 값이 음수인 경우, 두 변수가 음의 상관관계를 가지고 있음을 나타냅니다. 값이 0에 가까울수록 두 변수 간의 선형적인 관계가 약하다는 의미입니다.

    - 상관계수는 공분산을 각 변수의 표준편차로 나눈 값으로, 두 변수 간의 선형적인 관계의 강도와 방향을 나타냅니다. 상관계수는 -1에서 1 사이의 값을 가지며, 수식으로 표현하면 다음과 같습니다.

        - ρ(X, Y) = Cov(X, Y) / (σX * σY)

        - ρ는 "로"라고 읽으며, 일반적으로 상관계수를 나타내는 기호입니다. σ는 "시그마"라고 읽으며, 일반적으로 표준편차를 나타내는 기호입니다. ρ(X, Y)는 변수 X와 Y의 상관계수를 나타내며, Cov(X, Y)는 공분산, σX는 변수 X의 표준편차, σY는 변수 Y의 표준편차를 의미합니다.

        - 상관계수가 1에 가까울수록 두 변수는 강한 양의 선형 관계를 가지고 있으며, -1에 가까울수록 강한 음의 선형 관계를 가지고 있습니다. 0에 가까울수록 두 변수 간의 선형적인 관계가 약하거나 없음을 나타냅니다.

    - 공분산과 상관계수는 두 변수 간의 관계를 분석하고 해석하는 데 도움을 주는 통계적인 지표입니다. 상관계수는 공분산보다 두 변수의 척도에 의존하지 않으므로, 변수의 척도가 다를 경우에도 비교가 용이합니다. 상관계수는 데이터 분석, 회귀 분석, 포트폴리오 관리 등 다양한 분야에서 활용됩니다.


- 신뢰 구간의 정의는 무엇인가요?

    - 신뢰 구간은 통계적 추정에서 사용되는 개념으로, 표본 데이터를 기반으로 모집단의 모수를 추정하고자 할 때 사용됩니다. 신뢰 구간은 추정된 값이 모수를 포함할 것으로 예상되는 범위를 나타냅니다.

    - 일반적으로, 신뢰 구간은 "추정값 ± 오차 범위"로 표현됩니다. 이때 오차 범위는 신뢰 수준에 따라 결정되며, 신뢰 수준이 높을수록 추정값이 포함될 가능성이 더 높아집니다. 예를 들어, 95% 신뢰 구간은 추정된 값이 실제 모수를 포함할 확률이 95%라는 의미입니다. 즉, 100번 중 약 95번의 경우 실제 모수가 해당 구간에 속하게 됩니다.
    
    - 신뢰 구간은 통계적 추정의 불확실성을 고려하여 신뢰성 있는 결론을 도출하는 데 사용됩니다. 이를 통해 모수에 대한 예측과 해석을 할 수 있게 됩니다.


- p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?

    - p-value는 가설 검정에서 사용되는 통계적인 지표로, 귀무가설(null hypothesis)에 대한 데이터의 일치 정도를 나타냅니다. p-value는 특정 가설을 검정했을 때, 해당 가설이 참일 때 우연히 얻은 데이터보다 더 극단적인 결과가 관측될 확률을 의미합니다.

    - 가설 검정에서는 일반적으로 귀무가설과 대립가설(alternative hypothesis)을 설정합니다. 귀무가설은 어떤 원인이나 영향이 없다는 가정을 의미하고, 대립가설은 어떤 원인이나 영향이 있다는 가정을 의미합니다. p-value는 귀무가설이 맞을 때 우리가 얻은 결과보다 더 극단적인 결과가 나올 확률을 계산하여 제시합니다.

    - p-value의 값은 0과 1 사이의 범위를 가지며, 일반적으로 0.05 (또는 0.01)보다 작으면 우리는 귀무가설을 기각하고 대립가설을 받아들입니다. 이는 해당 결과가 우연히 발생할 확률이 매우 작기 때문에 다른 원인 또는 영향이 있는 것으로 해석할 수 있기 때문입니다.

    - p-value는 가설 검정에서 우리가 얻은 데이터로부터 귀무가설을 평가하고 결론을 도출하는 데에 사용되는 중요한 지표입니다.


- R square의 의미는 무엇인가요?

    - R-square는 회귀 분석에서 사용되는 통계적인 지표로, 종속 변수의 총 변동성 중 독립 변수들에 의해 설명되는 변동성의 비율을 나타냅니다.

    - 회귀 분석은 종속 변수와 독립 변수들 간의 관계를 모델링하는 분석 방법입니다. 이때, 종속 변수의 변동성은 독립 변수들에 의해 설명될 수 있고, 남아 있는 변동성은 모델로 설명되지 않는 잔차로 남게 됩니다. R-square는 이러한 설명된 변동성의 비율을 측정하여 모델의 적합도를 평가하는 지표입니다.

    - R-square 값은 0부터 1까지의 범위를 가지며, 1에 가까울수록 모델이 종속 변수의 변동성을 더 잘 설명한다는 의미입니다. 0이라면 모델이 종속 변수의 변동성을 전혀 설명하지 못하는 것을 의미합니다.

    - 하지만 R-square는 모델의 적합도만을 평가하고, 다른 요인들을 고려하지 않습니다. 따라서 다른 통계적인 지표와 함께 고려하여 모델의 성능을 종합적으로 평가하는 것이 중요합니다.


- 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?

    - 평균과 중앙값은 데이터의 중심 경향성을 나타내는 통계량입니다. 어떤 케이스에서 평균을 사용하고, 어떤 케이스에서 중앙값을 사용해야 하는지에 대해 설명드리겠습니다.

    - 평균(mean): 평균은 데이터의 모든 값을 더한 후 전체 데이터 개수로 나눈 값입니다. 평균은 데이터의 대표값을 계산하는 데에 가장 일반적으로 사용됩니다. 평균은 데이터의 분포를 고려하여 계산되기 때문에 이상치(outlier)에 영향을 받을 수 있습니다. 따라서 데이터가 이상치를 포함하고 있을 때에는 평균을 사용하기 전에 이상치의 영향을 조사하고 적절한 조치를 취해야 합니다. 또한, 데이터가 정규분포와 같은 대칭적인 분포를 따를 때 평균은 중앙값과 유사한 값을 가집니다.

    - 중앙값(median): 중앙값은 데이터를 크기순으로 정렬했을 때 가운데에 위치한 값입니다. 중앙값은 이상치에 영향을 받지 않기 때문에 데이터에 이상치가 포함되어 있을 경우에는 평균보다 신뢰성이 높은 대표값으로 사용될 수 있습니다. 또한, 데이터가 비대칭적인 분포를 가지거나 이상치가 많은 경우에는 중앙값이 데이터의 경향성을 더 잘 반영할 수 있습니다.

    - 데이터의 특성과 목적에 따라 평균과 중앙값 중 어떤 값을 사용할지 결정해야 합니다. 일반적으로 대부분의 경우에서 평균이 사용되지만, 이상치의 영향을 최소화하고자 할 때나 비대칭적인 분포를 가지는 데이터에 대해서는 중앙값이 더 적합한 대표값으로 선택될 수 있습니다.


- 중심극한정리는 왜 유용한걸까요?

    - "독립적인 확률 변수들의 합이 극한을 향해 정규분포를 따른다."
    - 중심극한정리는 여러 개의 독립적인 확률 변수의 합이 정규분포에 근사하는 현상을 설명하는데 사용됩니다. 이는 다음과 같은 이점을 제공합니다.

        - 대표성: 중심극한정리에 따라, 많은 독립적인 확률 변수들의 합은 정규분포에 근사하기 때문에, 다양한 확률 분포를 가진 모집단에서 추출한 표본들의 합은 대부분 정규분포를 따릅니다. 이를 통해 표본의 평균이 모집단의 평균에 대한 좋은 추정값이 되는 경우가 많습니다.

        - 통계적 추론: 중심극한정리는 통계적 추론에서 매우 중요한 역할을 합니다. 모집단의 분포가 어떤 형태인지 명확히 알 수 없는 경우에도, 중심극한정리를 사용하여 표본의 평균이 정규분포를 따른다고 가정할 수 있습니다. 이를 바탕으로 표본의 평균을 이용하여 모집단의 평균을 추론하거나 가설 검정을 수행할 수 있습니다.

        - 확률 분포의 근사: 중심극한정리는 다양한 분포를 가진 확률 변수들이 정규분포에 근사한다는 것을 보여줍니다. 따라서, 중심극한정리를 이용하여 어떤 확률 변수의 분포를 정규분포로 근사화하여 계산하거나 추정할 수 있습니다. 이를 통해 확률적인 문제를 보다 간편하게 다룰 수 있게 됩니다.

    - 중심극한정리는 통계학의 기본 개념이며 다양한 분야에서 활용됩니다. 모집단 분포가 무엇이든지 상관없이, 표본의 크기가 충분히 크다면 중심극한정리를 사용하여 정규분포를 가정할 수 있으며, 이는 통계적 분석과 추론에 큰 도움을 줍니다.


- 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.

    - 엔트로피(Entropy)는 정보 이론에서 사용되는 개념으로, 어떤 확률 분포의 불확실성 또는 정보의 혼잡도를 나타내는 척도입니다. 엔트로피는 확률 분포의 불확실성 정도를 수량화하는데 사용되며, 정보 이론, 통계, 머신러닝 등 다양한 분야에서 중요한 개념입니다.

    - 확률 분포가 가지는 엔트로피 값은 해당 분포의 가능한 결과들이 얼마나 균등하게 분포되어 있는지를 나타냅니다. 엔트로피 값이 높을수록 분포가 더 불확실하며, 값이 낮을수록 분포가 더 확실한 것을 의미합니다.수식적으로 엔트로피는 다음과 같이 정의됩니다.

        - H(X) = -Σ P(x) * log₂(P(x))

        - 여기서 H(X)는 확률 변수 X의 엔트로피를 나타내며, P(x)는 각각의 결과 x가 발생할 확률을 의미합니다. log₂는 밑이 2인 로그 함수를 의미합니다. 엔트로피는 확률 변수의 가능한 결과들에 대해 확률과 로그 확률의 곱을 합하여 계산됩니다.

    - Information Gain(정보 이득)은 엔트로피의 개념을 활용하여 머신러닝에서 사용되는 개념입니다. Information Gain은 주어진 특징(feature)이 얼마나 분류 작업(classification)에 도움이 되는지를 측정하는 데 사용됩니다. 주어진 특징을 기준으로 데이터를 분할했을 때, 분할 이전과 분할 이후의 엔트로피 차이를 측정하여 정보 이득을 계산합니다. 정보 이득이 크다는 것은 특징을 사용하면 분류 작업이 더 잘 이루어진다는 것을 의미합니다.

    - Information Gain은 엔트로피를 기반으로 하는데, 특징을 사용하여 분할했을 때 엔트로피의 감소를 나타냅니다. 정보 이득이 큰 특징은 더 유용한 특징으로 간주되며, 이를 통해 데이터를 분류하거나 특징 선택(feature selection)에 사용됩니다.


- 어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?

    - 모수적 방법론과 비모수적 방법론은 통계 분석에서 데이터의 특성과 가정에 따라 선택됩니다.

    - 모수적 방법론은 통계 모델의 분포에 대한 가정을 전제로 하고, 모델의 모수(parameter)를 추정하는 방법입니다. 이 방법론은 데이터가 특정 분포를 따른다고 가정하고, 분포의 모수를 추정하여 모델링을 수행합니다. 예를 들어, 데이터가 정규 분포를 따른다고 가정하고 평균과 분산을 추정하는 등의 작업을 수행할 수 있습니다. 모수적 방법론은 가정이 잘 맞는 경우에 유용하며, 모델이 간단하고 파라미터의 개수가 적을 때 적합합니다. 그러나 데이터의 분포에 대한 가정이 잘못되었을 경우 추정 결과가 부정확할 수 있습니다.

    - 비모수적 방법론은 분포에 대한 가정을 하지 않고, 데이터의 분포를 직접 추정하는 방법입니다. 이 방법론은 모수적 방법론과 달리 데이터에 대한 분포 가정이 필요 없으므로 보다 유연하게 적용할 수 있습니다. 비모수적 방법론은 자유도가 높고 복잡한 분포를 다룰 수 있으며, 데이터의 분포에 대한 사전 지식이 없거나 가정이 어려운 경우에 적합합니다. 그러나 비모수적 방법론은 더 많은 데이터를 요구하고, 계산적으로 더 복잡할 수 있습니다.

    - 모수적 방법론과 비모수적 방법론의 선택은 데이터의 특성과 가정, 분석 목적에 따라 달라집니다. 가정이 적절하고 모델이 간단한 경우에는 모수적 방법론을 사용하고, 가정이 어려우거나 자유도가 높은 추정이 필요한 경우에는 비모수적 방법론을 사용할 수 있습니다.


- “likelihood”와 “probability”의 차이는 무엇일까요?

    - "Likelihood"와 "probability"는 통계학에서 사용되는 두 용어로, 확률과 관련이 있지만 의미와 사용 방법이 다릅니다.

    - "Probability(확률)"는 주어진 사건이 발생할 가능성을 나타내는 숫자입니다. 일반적으로 주어진 확률 분포에 따라 사건이 발생할 확률을 계산합니다. 예를 들어, 동전 던지기에서 앞면이 나올 확률이 0.5라면, 확률적으로 앞면이 나올 가능성은 0.5입니다. 확률은 일반적으로 사전에 정의된 확률 분포를 기반으로 계산됩니다.

    - "Likelihood(우도)"는 주어진 데이터가 주어진 모델에 대해 얼마나 "가능한"지를 나타내는 개념입니다. 우도는 주어진 모델의 파라미터 값을 사용하여 주어진 데이터가 관찰될 확률을 계산합니다. 주어진 데이터에 대해 우도가 높을수록 해당 모델의 파라미터 값이 데이터에 잘 적합된다고 말할 수 있습니다.

    - 간단히 말하면, 확률은 주어진 모델의 파라미터 값을 알 때 사건이 발생할 가능성을 나타내는 반면, 우도는 주어진 데이터가 주어진 모델에서 발생할 가능성을 나타냅니다. 확률은 모델과 함께 사용되는 반면, 우도는 데이터와 함께 사용됩니다.

    - 예를 들어, 동전 던지기에서 앞면이 나올 확률이 0.5인 모델이 있을 때, 앞면이 나온 10번의 관측 데이터가 주어졌다고 가정해 봅시다. 이 데이터가 주어진 모델에 대해 얼마나 "가능한"지를 계산하는 것이 우도입니다. 즉, 주어진 모델에서 앞면이 나온 10번의 데이터가 관찰될 확률을 계산하여 해당 모델에 대한 우도를 구할 수 있습니다.


- 통계에서 사용되는 bootstrap의 의미는 무엇인가요.

    - 부트스트래핑(Bootstrap)은 통계적인 추론과 모델 검정에서 사용되는 비모수적인 방법 중 하나입니다. 부트스트래핑은 주어진 샘플 데이터를 기반으로 모집단에서의 통계적 특성을 추정하기 위해 사용됩니다.

    - 일반적인 통계적 추론 방법에서는 모집단의 분포나 모수에 대한 가정이 필요합니다. 하지만 실제 데이터의 분포가 알려지지 않거나 가정하기 어려운 경우에는 부트스트래핑을 사용하여 추정을 수행할 수 있습니다. 부트스트래핑은 다음과 같은 과정으로 이루어집니다.

        - 주어진 샘플 데이터에서 복원추출을 통해 임의의 크기의 부트스트랩 샘플을 생성합니다. 이때, 부트스트랩 샘플의 크기는 원래 데이터의 크기와 동일하게 설정됩니다.

        - 생성된 부트스트랩 샘플을 기반으로 통계적 추정량(예: 평균, 분산, 상관계수 등)을 계산합니다.

    - 위 과정을 여러 번 반복하여 여러 개의 부트스트랩 샘플을 생성하고, 각 샘플에서의 추정량을 얻습니다.

    - 추정량들의 분포나 변동성을 통해 모수에 대한 신뢰구간을 추정하거나, 가설 검정을 수행할 수 있습니다.

    - 부트스트래핑은 모수적 가정이 필요 없으며, 비모수적인 방법으로 모집단의 특성을 추정할 수 있는 장점이 있습니다. 특히, 표본이 작거나 비정규분포인 경우에 유용하게 사용됩니다.


- 모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?

    - 모수가 매우 적은 케이스에서는 일반적으로 모델의 복잡성을 줄이고, 과적합을 방지하기 위해 간단한 모델을 활용하는 것이 일반적입니다. 몇 가지 대표적인 방법을 아래에 설명하겠습니다.

       - 단순한 통계 모델: 모델의 복잡성을 낮추기 위해 단순한 통계 모델을 사용할 수 있습니다. 예를 들어, 평균이나 중앙값을 사용하여 예측하는 간단한 모델을 적용할 수 있습니다.

       - 기본 알고리즘 활용: 일반적인 머신러닝 알고리즘 중에서도 파라미터 수가 적은 모델을 선택할 수 있습니다. 선형 회귀, 로지스틱 회귀, 의사결정트리 등은 매개변수가 적은 모델로 예측을 수행할 수 있는 대표적인 예입니다.

       - 피쳐 선택 및 차원 축소: 변수가 적은 경우, 피쳐 선택이나 차원 축소 기법을 사용하여 모델에 사용할 변수를 선택할 수 있습니다. 변수 선택은 예측에 가장 유의미한 변수를 선택하고, 차원 축소는 변수들 간의 상관 관계를 고려하여 데이터를 잘 설명할 수 있는 적은 수의 주요 변수로 축소하는 방법입니다.

       - 교차 검증과 하이퍼파라미터 튜닝: 모델의 일반화 성능을 평가하기 위해 교차 검증을 수행하고, 최적의 하이퍼파라미터를 선택하기 위해 그리드 서치나 랜덤 서치 등의 방법을 활용할 수 있습니다.

       - 앙상블 모델: 모델 성능을 향상시키기 위해 앙상블 모델을 활용할 수 있습니다. 예를 들어, 다수의 단순한 모델을 결합하여 예측을 수행하는 앙상블 방법인 배깅(Bagging)이나 부스팅(Boosting)을 활용할 수 있습니다.

   - 위의 방법들은 모델의 복잡성을 제어하고, 적은 모수의 경우에도 적절한 예측 모델을 수립할 수 있는 방법들입니다. 하지만 데이터의 특성과 목표에 따라 최적의 방법이 달라질 수 있으므로, 상황에 맞게 선택해야 합니다.


- 베이지안과 프리퀀티스트 간의 입장차이를 설명해주실 수 있나요?

    - 베이지안과 프리퀀티스트는 통계적 추론에 대한 다른 접근 방식을 가지고 있는 입장입니다. 이들 사이에는 다음과 같은 주요한 입장 차이가 있습니다.

        - 사전 지식의 활용: 베이지안은 사전 지식을 추론에 포함시키는 것을 주장합니다. 사전 지식은 사전 분포로 표현되며, 데이터를 통해 업데이트된 사후 분포를 얻습니다. 이는 주관적인 사전 지식이 추론에 영향을 미칠 수 있다는 점에서 프리퀀티스트와 다릅니다. 반면에 프리퀀티스트는 사전 지식을 고려하지 않고, 주어진 데이터만으로 추론을 수행합니다.

        - 확률의 해석: 베이지안은 확률을 주관적인 믿음의 정도로 해석합니다. 베이지안 추론은 확률 분포를 통해 불확실성을 모델링하고, 추론 결과도 확률적인 형태로 제시됩니다. 이에 반해 프리퀀티스트는 확률을 반복된 무작위 실험의 빈도로 해석합니다. 추론 결과는 신뢰도나 신뢰구간 등의 형태로 나타납니다.

        - 가설 검정: 베이지안은 가설 검정을 확률적인 기준으로 수행합니다. 가설에 대한 사후 확률을 계산하여 해당 가설의 지지 여부를 판단합니다. 프리퀀티스트는 가설 검정을 통계적 유의성과 가설의 기각/채택 여부로 판단합니다.

        - 모수 추정: 베이지안은 모수에 대한 사후 분포를 추정하여 모델링합니다. 이는 추정된 모수의 불확실성을 나타내는 것입니다. 프리퀀티스트는 모수에 대한 점 추정치를 계산하며, 이를 통해 모델링합니다.

    - 각각의 입장은 장단점을 가지고 있고, 특정 상황이나 문제에 따라 적합한 접근 방식을 선택할 수 있습니다. 이들 사이에는 이론적인 차이와 방법론의 차이가 있기 때문에, 통계적 추론에서 이러한 입장 차이를 이해하고 적절한 접근을 선택하는 것이 중요합니다.


- 검정력(statistical power)은 무엇일까요?

    - 검정력(statistical power)은 통계적 가설 검정에서 중요한 개념으로, 특정 가설 검정의 민감도와 관련이 있습니다. 검정력은 "귀무가설이 잘못된 경우를 올바르게 기각할 확률"로 정의할 수 있습니다.

    - 검정력은 주로 다음과 같은 상황에서 중요하게 고려됩니다.

        - 대립가설의 참인 경우를 식별하고자 할 때: 가설 검정에서 대립가설은 보통 연구자가 관심을 가지는 가설이며, 검정력은 이 대립가설이 참일 때 올바르게 가설을 기각할 확률을 나타냅니다.

        - 표본 크기 결정: 검정력은 표본 크기 결정에 영향을 미칩니다. 충분한 검정력을 확보하기 위해 필요한 표본 크기를 결정할 때 사용됩니다. 검정력이 낮으면 작은 효과를 검출할 수 있는 능력이 감소하며, 표본 크기가 커질수록 검정력은 향상됩니다.

    - 검정력은 주로 다음과 같은 요소에 영향을 받습니다.

        - 효과 크기: 검정력은 효과 크기와 관련이 있습니다. 효과 크기가 클수록 검정력은 증가합니다.

        - 유의 수준: 검정력은 유의 수준과 관련이 있습니다. 유의 수준이 높을수록 검정력은 증가합니다.

        - 표본 크기: 검정력은 표본 크기와 관련이 있습니다. 표본 크기가 크면 검정력이 증가합니다.

    - 검정력은 가설 검정의 신뢰성과 유의성을 평가하는 데 중요한 지표입니다. 충분한 검정력을 확보하여 원하는 대립가설을 올바르게 식별할 수 있는지 확인하는 것이 중요합니다.


- missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?

    - Missing value(결측값)이 있는 경우에는 채워야 할 필요가 있을 수 있습니다. 이는 다음과 같은 이유로 인해 중요합니다.

        - 통계적 분석: 결측값이 있는 데이터를 그대로 사용하면 분석 결과의 정확성과 신뢰성이 저하될 수 있습니다. 결측값이 있는 데이터를 적절히 처리하고 채워주면 통계적 분석에 활용할 수 있는 데이터셋을 구성할 수 있습니다.

        - 데이터 왜곡 방지: 결측값이 있는 경우, 해당 변수의 분포나 관계를 왜곡시킬 수 있습니다. 채워진 데이터를 사용하여 변수 간의 관계를 보다 정확하게 파악할 수 있습니다.

        - 효율적 데이터 활용: 결측값을 적절히 채워주면 데이터셋의 활용도가 높아집니다. 결측값을 처리하지 않으면 해당 샘플이 분석에서 제외되거나, 해당 변수가 활용되지 않는 등의 데이터의 손실이 발생할 수 있습니다.

        - 예측 및 모델링: 결측값을 적절히 채워주면 예측 모델링에서 더 나은 결과를 얻을 수 있습니다. 결측값이 있는 경우, 해당 샘플을 예측 모델링에서 활용하기 위해 채워주는 것이 중요합니다.

    - 결측값을 채우는 방법은 여러 가지가 있으며, 데이터의 특성과 분석 목적에 따라 선택됩니다. 일반적인 방법으로는 대체값(예: 평균, 중앙값, 최빈값), 회귀 분석에 기반한 예측값, 다중 대체법(Multiple Imputation) 등이 사용될 수 있습니다. 결측값을 채워줄 때는 데이터의 왜곡을 방지하고, 통계적으로 타당한 방법을 사용하여 적절한 대체값을 도출하는 것이 중요합니다.


- 아웃라이어의 판단하는 기준은 무엇인가요?

    - 아웃라이어를 판단하는 기준은 데이터의 분포와 통계적 특성에 기반하여 정의됩니다. 다음은 몇 가지 일반적인 기준과 방법을 설명합니다.

    - 통계적 기준:

        - Z-Score: Z-Score는 데이터 포인트가 평균으로부터 몇 표준편차만큼 떨어져 있는지를 나타내는 값입니다. 일반적으로 Z-Score의 절대값이 3 이상인 데이터 포인트는 이상치로 간주될 수 있습니다.

        - 사분위수 범위(IQR): IQR은 데이터의 상위 75%와 하위 25% 사이의 범위를 나타내는 값입니다. 일반적으로 IQR을 1.5배 이상 벗어나는 값들은 이상치로 간주될 수 있습니다.

    - 상자 그림(Box Plot):

        - 상자 그림은 데이터의 중앙값, 사분위수, 이상치 등을 시각적으로 나타내는 그래프입니다. 상자 그림에서 일반적인 상자의 범위를 벗어나는 값들은 이상치로 간주될 수 있습니다.

    - 도메인 지식:

        - 데이터 분석을 수행하는 도메인의 전문적인 지식을 활용하여 이상치를 판단할 수 있습니다. 예를 들어, 특정 값이 실제로 가능하지 않은 범위에 존재한다거나, 도메인의 특수한 제약 조건을 위반하는 값이라고 판단되는 경우 이상치로 간주할 수 있습니다.

    - 기계 학습:

        - 기계 학습 알고리즘을 사용하여 이상치를 탐지하는 방법도 있습니다. 예를 들어, 클러스터링 알고리즘 중 밀도 기반 이상치 탐지(DBSCAN) 알고리즘은 데이터의 밀도에 기반하여 이상치를 식별할 수 있습니다.

    - 이외에도 이상치를 판단하는 다양한 통계적 기법과 알고리즘이 존재합니다. 이상치의 판단은 데이터의 특성과 분석 목적에 따라 다를 수 있으며, 여러 가지 기준을 조합하여 사용하기도 합니다. 중요한 것은 이상치를 신중하게 판단하고, 분석 목적에 부합하는 방식으로 처리하는 것입니다.


- 필요한 표본의 크기를 어떻게 계산합니까?

    - 표본의 크기를 계산하는 방법은 분야와 분석 목적에 따라 다를 수 있습니다. 일반적으로 표본 크기는 다음과 같은 요소들을 고려하여 결정됩니다.

        - 효과 크기 (Effect Size): 연구에서 관심 있는 효과의 크기가 클수록 더 큰 표본이 필요합니다. 작은 효과를 감지하려면 더 많은 데이터가 필요합니다.

        - 유의 수준 (Significance Level): 연구에서 설정한 유의 수준에 따라 표본 크기가 달라질 수 있습니다. 일반적으로 보통 유의 수준은 0.05 (또는 0.01)로 설정되며, 유의 수준이 낮을수록 더 큰 표본이 필요합니다.

        - 검정력 (Power): 연구에서 원하는 검정력에 따라 표본 크기가 달라집니다. 검정력은 효과를 정확하게 검출할 수 있는 능력을 나타내며, 일반적으로 0.8 이상의 검정력을 원합니다.

        - 변동성 (Variability): 연구에서 측정하는 변수의 변동성이 클수록 더 큰 표본이 필요합니다. 데이터의 분산이 크면 효과를 신뢰할 수 있는 정도를 확보하기 위해 더 많은 데이터가 필요합니다.

    - 표본 크기를 계산하기 위해 위의 요소들을 고려하여 통계 계산이나 샘플 크기 계산 방법론을 사용할 수 있습니다. 예를 들어, t-검정의 경우에는 효과 크기, 유의 수준, 검정력 등을 고려하여 표본 크기를 계산하는 방법이 있습니다. 다양한 통계 소프트웨어나 온라인 툴을 활용하여 표본 크기를 계산할 수도 있습니다.

    - 중요한 점은 표본 크기를 충분히 계산하여 연구의 목적을 달성하기 위해 데이터 수집을 잘 설계하는 것입니다. 부적절한 표본 크기로 인해 검정력이 낮아진다면 유의미한 결과를 얻기 어렵거나 잘못된 결론을 도출할 수 있습니다. 따라서 표본 크기 계산은 연구 설계의 중요한 부분이며, 충분한 표본 크기를 확보하는 것이 중요합니다.


- Bias를 통제하는 방법은 무엇입니까?

    - Bias를 통제하기 위해 다음과 같은 방법들을 사용할 수 있습니다.

        - 적절한 변수 선택: 모델에 포함되는 변수들을 신중하게 선택하는 것이 중요합니다. 모델에 불필요한 변수를 포함하면 모델의 복잡성이 증가하고, 이는 bias를 증가시킬 수 있습니다. 따라서 주요한 변수들을 선택하여 모델을 단순화하는 것이 중요합니다.

        - 모델의 복잡성 조절: 모델의 복잡성은 bias와 trade-off 관계에 있습니다. 복잡한 모델은 훈련 데이터에 과도하게 적합될 수 있으며, 이는 bias를 줄이지만 variance를 증가시킬 수 있습니다. 모델의 복잡성을 적절히 조절하여 bias와 variance 사이의 균형을 맞추는 것이 중요합니다.

        - 큰 규모의 데이터 사용: 더 많은 데이터를 사용하면 모델이 더 정확한 예측을 할 수 있습니다. 충분한 크기의 데이터를 사용하면 표본의 다양성이 증가하고, 이는 bias를 감소시킬 수 있습니다.

        - Cross-validation: Cross-validation은 모델의 성능을 평가하고 튜닝하는 데 도움이 됩니다. 데이터를 여러 개의 폴드로 나누어 모델을 훈련하고 평가하는 과정을 반복하여 모델의 일반화 성능을 추정합니다. Cross-validation을 통해 모델이 훈련 데이터에 과적합되지 않도록 조절할 수 있습니다.

        - Regularization: Regularization은 모델의 복잡성을 제어하는 방법 중 하나입니다. Regularization은 모델의 가중치를 제한함으로써 모델의 복잡성을 감소시킵니다. L1 regularization (Lasso) 또는 L2 regularization (Ridge)와 같은 regularization 기법을 사용하여 bias를 조절할 수 있습니다.

        - 앙상블 모델: 앙상블 모델은 여러 개의 모델을 조합하여 예측을 수행하는 방법입니다. 앙상블은 다양한 모델의 예측을 결합함으로써 bias를 줄이고 정확성을 향상시킬 수 있습니다. 대표적인 앙상블 기법으로는 랜덤 포레스트(Random Forest)나 그래디언트 부스팅(Gradient Boosting) 등이 있습니다.

    - 이러한 방법들은 bias를 통제하여 모델의 일반화 성능을 개선할 수 있습니다. 그러나 각 상황과 데이터에 따라 적절한 방법을 선택하는 것이 중요합니다.


- 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.

    - 로그 함수는 다양한 분야에서 유용하게 사용됩니다. 몇 가지 예시를 들어 설명해드리겠습니다.

        - 데이터 스케일 조정: 데이터의 범위가 넓을 때 로그 함수를 사용하여 스케일을 조정할 수 있습니다. 예를 들어, 수입 데이터의 경우 수입이 매우 다양한 범위에 분포할 수 있습니다. 이때 로그 함수를 적용하면 데이터의 분포를 더 균일하게 만들 수 있습니다.

        - 이벤트 발생률 분석: 로그 함수는 이벤트 발생률의 패턴을 분석하는 데 유용합니다. 예를 들어, 광고 클릭 수, 웹 사이트 방문 수 등의 이벤트가 시간에 따라 변하는 경우 로그 함수를 사용하여 이벤트 발생률의 트렌드를 보다 잘 파악할 수 있습니다.

        - 확률 계산: 로그 함수는 확률 계산에도 자주 사용됩니다. 확률은 종종 매우 작은 값으로 표현되는데, 로그 함수를 적용하면 확률 값을 편리하게 계산할 수 있습니다. 또한 로그 함수는 확률의 곱셈을 덧셈으로 변환하여 계산을 간소화하는 데 도움을 줍니다.

        - 정보 이론: 로그 함수는 정보 이론에서 중요한 개념인 엔트로피와 관련이 있습니다. 엔트로피는 정보의 불확실성을 나타내는 척도로 사용되며, 로그 함수를 이용하여 엔트로피를 계산할 수 있습니다.

    - 이외에도 로그 함수는 확률론, 통계, 경제학, 금융 등 다양한 분야에서 활용됩니다. 로그 함수는 데이터나 문제의 특성에 따라 유용하게 적용될 수 있으며, 해당 분야의 문제를 해결하는 데 도움을 줄 수 있습니다.


- 베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포에 대해 설명해주세요. 그리고 분포 간의 연관성도 설명해주세요.

    - 분포들에 대한 간단한 설명과 분포들 간의 연관성에 대해 알려드리겠습니다.

        - 베르누이 분포: 이항 분포의 특수한 경우로, 단 하나의 이진 결과를 가지는 확률 변수에 적용됩니다. 성공과 실패, 참과 거짓 등 두 가지 가능한 결과가 있는 경우 사용됩니다.

        - 이항 분포: 베르누이 시행을 여러 번 반복하여 관심 있는 사건의 성공 횟수를 나타내는 분포입니다. 이항 분포는 성공 확률 p와 시행 횟수 n에 따라 정의됩니다.

        - 카테고리 분포: 범주형 데이터를 모델링하는데 사용되는 분포입니다. 카테고리 분포는 여러 개의 범주 중 하나를 선택하는 확률 변수에 적용됩니다. 주사위를 던져 나오는 눈의 숫자, 색상 선택 등이 카테고리 분포의 예시입니다.

        - 다항 분포: 동시에 여러 개의 카테고리를 가지는 확률 변수에 적용됩니다. 다항 분포는 각 카테고리의 확률 벡터를 나타내며, 다항 분포의 매개 변수로는 각 카테고리의 확률 분포를 나타내는 벡터가 사용됩니다.

        - 가우시안 정규 분포: 연속적인 실수 값을 가지는 확률 변수에 가장 널리 사용되는 분포입니다. 종 모양의 분포로서 평균과 분산에 의해 완전히 정의됩니다. 중심극한정리에 따라 많은 독립적인 확률 변수들의 합으로 근사할 수 있습니다.

        - t 분포: 작은 샘플 크기에서 모집단의 평균에 대한 추론을 수행할 때 사용되는 분포입니다. t 분포는 정규 분포의 모평균에 대한 검정에 활용되며, 샘플 크기가 작을 때 정확한 추정을 제공합니다.

        - 카이제곱 분포: 카이제곱 통계량을 따르는 확률 변수의 분포입니다. 주로 분할표 분석이나 독립성 검정에 사용됩니다.

        - F 분포: 두 개 이상의 모집단 분산의 비율을 검정하는 데 사용되는 분포입니다. 주로 분산 분석(ANOVA)에서 활용됩니다.

        - 베타 분포: 0과 1 사이의 값을 가지는 확률 변수의 분포를 모델링하는데 사용됩니다. 베타 분포는 베르누이 분포의 모수인 성공 횟수와 실패 횟수를 모델링할 때 사용됩니다.

        - 감마 분포: 양수 값을 가지는 연속 확률 변수의 분포를 나타냅니다. 감마 분포는 시간, 수익, 수명 등 양수 값의 특성을 모델링하는 데 사용됩니다.

    - 이러한 분포들은 통계학과 확률론에서 주로 사용되며, 서로 다른 분포들은 확률 변수의 특성과 모델링하는 데이터 유형에 따라 선택됩니다. 분포들 간에는 다양한 연관성과 관계가 존재합니다. 여기서는 몇 가지 대표적인 연관성에 대해 설명해드리겠습니다.

       - 중심극한정리: 중심극한정리는 독립적이고 동일한 분포를 가진 확률 변수들의 합이 정규 분포에 근사되는 현상을 말합니다. 즉, 여러 개의 독립적인 확률 변수들의 합이 정규 분포를 따른다는 것입니다. 중심극한정리는 표본 평균의 분포가 모집단이 정규 분포를 따르지 않아도 표본 크기가 충분히 크다면 정규 분포에 근사됨을 보장해줍니다.

       - 카이제곱 분포와 정규 분포의 관계: 카이제곱 분포는 독립적인 표준 정규 분포를 따르는 확률 변수들의 제곱의 합으로 정의됩니다. 따라서, 카이제곱 분포는 정규 분포와 밀접한 연관성을 가지고 있습니다. 카이제곱 분포는 주로 분할표 분석이나 분산 분석과 같은 통계적 검정에서 사용됩니다.

       - t 분포와 정규 분포의 관계: t 분포는 정규 분포와 유사하지만, 표본의 크기가 작을 때 정확한 추정을 제공하는데 사용됩니다. 작은 표본 크기에서 모집단의 평균에 대한 추론을 수행할 때, 정규 분포 대신 t 분포를 사용합니다.

       - 베르누이 분포와 이항 분포: 베르누이 분포는 단 하나의 이진 결과를 가지는 확률 변수에 적용되고, 이항 분포는 베르누이 시행을 여러 번 반복하여 관심 있는 사건의 성공 횟수를 나타냅니다. 이항 분포는 여러 개의 독립적인 베르누이 분포의 합으로 표현될 수 있습니다.

    - 이러한 분포들 간의 연관성은 확률론과 통계학의 이론과 방법들을 이해하는 데 중요한 역할을 합니다. 특정한 분포를 선택하고 사용함으로써 데이터의 특성을 더 정확하게 모델링하고, 추론과 예측을 수행할 수 있게 됩니다.


- 출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?

    - 이 문제는 베이즈 정리를 활용하여 해결할 수 있습니다.

    - 우선, 사건 A를 "실제로 비가 내린다"라고 정의하고, 사건 B를 "3명의 친구가 모두 '그렇습니다. 비가 내리고 있습니다'라고 말한다"라고 정의하겠습니다.

    - 주어진 정보에 따르면, 친구가 진실을 말할 확률(P(A))은 2/3이고, 거짓을 말할 확률(P(~A))은 1/3입니다. 또한, 친구들의 발언이 독립적이라고 가정할 수 있으므로, P(B|A)는 2/3 * 2/3 * 2/3 = 8/27이 됩니다. 여기서 P(B)를 계산하기 위해 모든 가능한 경우를 고려해야 합니다.

    - 친구 3명이 모두 "그렇습니다. 비가 내리고 있습니다"라고 말한 경우, 이는 비가 내리는 경우와 비가 내리지 않는 경우 모두에 해당합니다. 즉, P(B)는 P(A)와 P(~A)에 대한 조건부 확률로 나타낼 수 있습니다.

    - P(B) = P(B|A) * P(A) + P(B|~A) * P(~A)

    - 여기서 P(B|~A)는 1/3 * 1/3 * 1/3 = 1/27이 됩니다. 따라서,

    - P(B) = (8/27 * 2/3) + (1/27 * 1/3) = 17/81

    - 따라서, 실제로 비가 내릴 확률인 P(A|B)는 베이즈 정리를 적용하여 계산할 수 있습니다.

    - P(A|B) = (P(B|A) * P(A)) / P(B)

    - P(A|B) = (8/27 * 2/3) / (17/81) ≈ 16/17

    - 즉, 친구들이 모두 "그렇습니다. 비가 내리고 있습니다"라고 말했을 때, 실제로 비가 내릴 확률은 약 16/17, 즉 약 0.9412입니다.



🤖 Machine Learning

- 알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)

    -


- 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?

    -


- Local Minima와 Global Minima에 대해 설명해주세요.

    -


- 차원의 저주에 대해 설명해주세요.

    -


- dimension reduction기법으로 보통 어떤 것들이 있나요?

    -


- PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?

    -


- LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?

    -


- Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?

    -


- 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?

    -


- SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

    -


- 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.

    -


- 회귀 / 분류시 알맞은 metric은 무엇일까?

    -


- Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.

    -


- 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?

    -


- 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

    -


- 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?

    -


- 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?

    -


- ROC 커브에 대해 설명해주실 수 있으신가요?

    -


- 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?

    -


- K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

    -


- L1, L2 정규화에 대해 설명해주세요.

    -


- Cross Validation은 무엇이고 어떻게 해야하나요?

    -


- XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

    -


- 앙상블 방법엔 어떤 것들이 있나요?

    -


- feature vector란 무엇일까요?

    -


- 좋은 모델의 정의는 무엇일까요?

    -


- 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?

    -


- 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?

    -


- OLS(ordinary least squre) regression의 공식은 무엇인가요?

    -



🧠 Deep Learning

- 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?

    -


- Cost Function과 Activation Function은 무엇인가요?

    -


- Tensorflow, PyTorch 특징과 차이가 뭘까요?

    -


- Data Normalization은 무엇이고 왜 필요한가요?

    -


- 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)

    -


- 오버피팅일 경우 어떻게 대처해야 할까요?

    -


- 하이퍼 파라미터는 무엇인가요?

    -


- Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?

    -


- 볼츠만 머신은 무엇인가요?

    -


- TF, PyTorch 등을 사용할 때 디버깅 노하우는?

    -


- 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?

    -


- 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?

    -


- Non-Linearity라는 말의 의미와 그 필요성은?

    -


- ReLU로 어떻게 곡선 함수를 근사하나?

    -


- ReLU의 문제점은?

    -


- Bias는 왜 있는걸까?

    -


- Gradient Descent에 대해서 쉽게 설명한다면?

    -


- 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?

    -


- GD 중에 때때로 Loss가 증가하는 이유는?

    -


- Back Propagation에 대해서 쉽게 설명 한다면?

    -


- Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?

    -


- GD가 Local Minima 문제를 피하는 방법은?

    -


- 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?

    -


- Training 세트와 Test 세트를 분리하는 이유는?

    -


- Validation 세트가 따로 있는 이유는?

    -


- Test 세트가 오염되었다는 말의 뜻은?

    -


- Regularization이란 무엇인가?

    -


- Batch Normalization의 효과는?

    -


- Dropout의 효과는?

    -


- BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?

    -


- GAN에서 Generator 쪽에도 BN을 적용해도 될까?

    -


- SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?

    -


- SGD에서 Stochastic의 의미는?

    -


- 미니배치를 작게 할때의 장단점은?

    -


- 모멘텀의 수식을 적어 본다면?

    -


- 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?

    -


- 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?

    -


- Back Propagation은 몇줄인가?

    -


- CNN으로 바꾼다면 얼마나 추가될까?

    -


- 간단한 MNIST 분류기를 TF, PyTorch 등으로 작성하는데 몇시간이 필요한가?

    -


- CNN이 아닌 MLP로 해도 잘 될까?

    -


- 마지막 레이어 부분에 대해서 설명 한다면?

    -


- 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?

    -


- 딥러닝할 때 GPU를 쓰면 좋은 이유는?

    -


- GPU를 두개 다 쓰고 싶다. 방법은?

    -


- 학습시 필요한 GPU 메모리는 어떻게 계산하는가?

    -
