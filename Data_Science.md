📈 Statistics/Math

- 고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?

    - 고유값(eigen value)과 고유벡터(eigen vector)는 선형 대수학에서 중요한 개념입니다. 이해하기 쉽게 설명해드리겠습니다.

    - 고유값(eigen value)은 선형 변환(행렬)이 고유벡터에 작용할 때, 해당 고유벡터 방향으로 크기를 변화시키는 비율입니다. 다시 말해, 행렬 A와 고유벡터 v가 주어졌을 때, A를 v에 곱한 결과가 고유값 λ와 v의 곱이 되는 것을 의미합니다. 수학적으로는 다음과 같은 식으로 표현됩니다: Av = λv.

    - 고유벡터(eigen vector)는 선형 변환(행렬)에 의해 크기만 변하고 방향은 변하지 않는 벡터입니다. 즉, 행렬 A를 곱했을 때 방향이 변하지 않는 벡터를 의미합니다. 고유벡터는 고유값에 대응되며, 하나 이상의 고유값에 대응하는 다양한 고유벡터가 존재할 수 있습니다.

    - 왜 고유값과 고유벡터가 중요할까요?

        - 변환의 특성 파악: 고유값과 고유벡터는 선형 변환을 이해하는 데 중요한 정보를 제공합니다. 행렬을 고유벡터로 변환하면, 해당 변환에 대한 특성을 파악할 수 있습니다. 예를 들어, 고유벡터는 회전이나 스케일링 변환의 축을 나타내며, 고유값은 그 변환의 크기를 나타냅니다.

        - 차원 축소: 고유값과 고유벡터는 차원 축소 기법에 활용됩니다. 대규모 데이터셋에서 차원이 매우 높을 때, 주요한 정보를 유지하면서 데이터를 저차원으로 압축하는 데 사용될 수 있습니다. 예를 들어, 주성분 분석(PCA)은 데이터를 고유벡터의 선형 조합으로 표현하여 차원을 축소합니다.

        - 행렬 분해: 고유값 분해(Eigenvalue decomposition)는 행렬을 고유값과 고유벡터로 분해하는 방법입니다. 이를 통해 행렬의 특성을 파악하고, 행렬 연산을 효율적으로 수행하거나, 선형 시스템의 해를 구하는 등 다양한 문제에 활용할 수 있습니다.

        - 네트워크 분석: 고유값과 고유벡터는 네트워크 분석에서 중요한 역할을 합니다. 예를 들어, 그래프의 인접 행렬에 대한 고유값과 고유벡터는 중요한 노드와 그룹을 식별하거나, 네트워크의 구조적 특징을 분석하는 데 사용될 수 있습니다.

    - 이렇듯 고유값과 고유벡터는 선형 대수학과 다양한 분야에서 중요한 개념으로 활용됩니다. 이해하고 활용할 수 있다면 데이터 분석과 모델링, 차원 축소, 네트워크 분석 등 다양한 문제를 더 깊이 이해하고 다룰 수 있을 것입니다.


- 샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?

    - 샘플링(Sampling)은 통계학과 데이터 분석에서 사용되는 중요한 개념입니다. 샘플링은 모집단(population)에서 일부 데이터를 추출하는 과정을 의미합니다. 추출된 데이터는 샘플(sample)이라고 부릅니다.

    - 리샘플링(Resampling)은 샘플 데이터에서 추가적인 추출을 통해 새로운 샘플을 생성하는 과정을 말합니다. 리샘플링은 주로 통계적 추론이나 모델 검증에 사용됩니다.

    - 리샘플링에는 주로 두 가지 기법이 사용됩니다:

        - 부트스트래핑(Bootstrapping): 부트스트래핑은 본래의 샘플 데이터에서 중복을 허용하여 새로운 샘플을 생성하는 방법입니다. 부트스트래핑은 통계량의 분포를 추정하거나 신뢰구간(confidence interval)을 구하는 데 사용됩니다. 복잡한 모델을 사용하지 않고도 데이터로부터 신뢰할 수 있는 통계적 추론을 할 수 있는 장점이 있습니다.

        - 재표본추출(Resampling): 재표본추출은 본래의 샘플 데이터에서 중복을 허용하지 않고 새로운 샘플을 생성하는 방법입니다. 이는 샘플의 크기를 유지하면서 데이터의 다양성을 조절할 수 있는 장점이 있습니다. 재표본추출은 분류 모델의 성능 평가나 교차 검증(cross-validation)에서 자주 사용됩니다.

    - 리샘플링의 장점:

        - 데이터의 불균형 대응: 리샘플링을 통해 데이터의 불균형 문제를 해결할 수 있습니다. 예를 들어, 이진 분류 문제에서 양성 클래스와 음성 클래스의 데이터 수가 차이가 많이 나는 경우, 리샘플링을 통해 데이터를 균형있게 만들어 모델의 학습을 개선할 수 있습니다.

        - 모델의 일반화 성능 향상: 리샘플링은 모델의 일반화 성능을 향상시키기 위해 사용될 수 있습니다. 부트스트래핑이나 재표본추출을 통해 생성된 샘플을 사용하여 모델을 다양하게 학습하고 평가함으로써, 모델의 안정성과 예측 성능을 높일 수 있습니다.

        - 통계적 추론의 유연성: 리샘플링은 통계적 추론에서 유연성을 제공합니다. 부트스트래핑이나 재표본추출을 통해 여러 번의 추출과 모델링을 수행함으로써, 추정된 통계량의 분포나 신뢰구간을 계산할 수 있습니다.

    - 요약하자면, 리샘플링은 샘플링 과정을 통해 데이터의 다양성을 확보하고 모델의 일반화 성능을 개선하기 위한 유용한 방법입니다. 데이터 분석에서 신뢰성 있는 통계적 추론을 수행하거나 모델의 성능을 평가하는 데 중요한 역할을 합니다.


- 확률 모형과 확률 변수는 무엇일까요?

    - 확률 모형과 확률 변수는 확률론에서 중요한 개념입니다.

    - 확률 변수(Probabilistic Variable)는 어떤 확률적인 현상을 나타내는 변수를 의미합니다. 이는 어떤 사건의 결과가 될 수 있는 값을 가지며, 그 값이 어떤 확률 분포에 의해 결정되는 변수입니다. 예를 들어, 동전 던지기를 확률 변수로 생각하면, 앞면이 나올 경우를 1, 뒷면이 나올 경우를 0으로 나타낼 수 있습니다. 확률 변수는 이산 확률 변수(Discrete Random Variable)와 연속 확률 변수(Continuous Random Variable)로 나눌 수 있습니다. 이산 확률 변수는 유한한 개수의 값을 가지며, 연속 확률 변수는 실수 범위에서 값을 가질 수 있습니다.

    - 확률 모형(Probabilistic Model)은 확률 변수의 동작이나 관계를 수학적으로 모델링한 것을 말합니다. 확률 모형은 확률 분포를 사용하여 확률 변수들 간의 관계를 설명하고 예측하는 데 사용됩니다. 확률 모형은 데이터를 통해 모델의 파라미터를 추정하거나, 주어진 모델을 사용하여 새로운 데이터를 생성하거나 예측하는 등 다양한 확률적인 추론 작업에 활용됩니다.

    - 확률 모형은 크게 두 가지 유형으로 나눌 수 있습니다:

        - 확률론적 모형(Probabilistic Models): 이러한 모형은 데이터의 생성 과정을 확률 분포를 통해 설명합니다. 대표적인 확률론적 모형으로는 가우시안 정규 분포, 이항 분포, 포아송 분포 등이 있습니다.

        - 그래픽 모형(Graphical Models): 그래픽 모형은 변수들 간의 관계를 그래프로 나타내어 표현합니다. 그래픽 모형은 베이지안 네트워크(Bayesian Networks)와 마코프 랜덤 필드(Markov Random Fields)와 같은 형태로 사용됩니다.

    - 확률 모형과 확률 변수는 확률론적인 사고와 추론에 중요한 개념으로 활용되며, 데이터 분석, 통계적 추론, 기계 학습 등 다양한 분야에서 사용됩니다.


- 누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요.

    - 누적 분포 함수(cumulative distribution function, CDF)와 확률 밀도 함수(probability density function, PDF)는 확률 분포를 표현하기 위해 사용되는 개념입니다.

    - 누적 분포 함수 (CDF):

        - 누적 분포 함수는 확률 변수 X가 특정 값보다 작거나 같은 확률을 나타내는 함수입니다. 수식으로는 다음과 같이 표현됩니다:

            - CDF(x) = P(X ≤ x)

            - 여기서 X는 확률 변수이고, x는 어떤 값입니다. 누적 분포 함수는 0에서 시작하여 1까지 증가하는 함수입니다. 확률 변수가 특정 값 이하의 값에 대해 얼마나 확률을 할당하는지를 나타냅니다.

    - 확률 밀도 함수 (PDF):

        - 확률 밀도 함수는 확률 변수의 값에 따른 확률 밀도를 나타내는 함수입니다. 수식으로는 다음과 같이 표현됩니다:

            - PDF(x) = d/dx[CDF(x)]

            - 여기서 d/dx는 도함수를 나타냅니다. 확률 밀도 함수는 확률 변수의 값에 대해 확률 밀도를 나타내는 함수로, 확률 변수가 특정 구간에 속할 확률은 해당 구간에서 확률 밀도 함수를 적분한 값으로 구할 수 있습니다.

    - 확률 분포에 따라 누적 분포 함수와 확률 밀도 함수의 수식이 달라지며, 각각의 분포마다 특정한 특성을 가지고 있습니다. 위의 수식은 일반적인 형태를 나타내며, 실제로는 확률 분포에 따라 다른 수식을 사용합니다.


- 조건부 확률은 무엇일까요?

    - 조건부 확률(Conditional Probability)은 하나의 사건이 다른 사건에 대해 발생할 확률을 나타내는 개념입니다. 어떤 사건 A가 일어났을 때, 다른 사건 B가 일어날 조건부 확률은 P(B|A)로 표기됩니다.

    - 조건부 확률은 다음과 같은 수식으로 정의됩니다:

        - P(B|A) = P(A∩B) / P(A)

        - 여기서 P(A∩B)는 사건 A와 B가 동시에 발생할 확률을, P(A)는 사건 A가 발생할 확률을 나타냅니다.

    - 조건부 확률은 주어진 조건 하에서 사건의 확률을 계산하는 데 사용됩니다. 예를 들어, 주어진 특성이나 정보에 기반하여 어떤 사건이 발생할 확률을 추정하고자 할 때 조건부 확률을 사용할 수 있습니다. 또한 조건부 확률은 베이즈 정리(Bayes' theorem)와 관련이 있어 확률적인 추론에 중요한 역할을 합니다.

    - 조건부 확률은 다양한 분야에서 사용됩니다. 예를 들어, 의료 진단에서 특정 증상이 나타났을 때 특정 질병에 걸릴 확률, 자연어 처리에서 이전 단어가 주어졌을 때 다음 단어의 확률 등을 계산하는 데 사용됩니다. 또한, 조건부 확률은 확률적인 모델링, 통계적 추론, 패턴 인식 등 다양한 분야에서 중요한 개념으로 활용됩니다.


- 공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요.

    - 공분산(Covariance)과 상관계수(Correlation Coefficient)는 두 변수 간의 관계를 측정하는 통계적인 개념입니다.

    - 공분산(Covariance):

        - 공분산은 두 변수 간의 상관 정도와 방향을 나타내는 값입니다. 두 변수 X와 Y의 공분산은 다음과 같이 정의됩니다:

            - Cov(X, Y) = E[(X - E[X])(Y - E[Y])]

            - 여기서 Cov는 공분산을 나타내며, E는 기대값(평균)을 나타냅니다. (X - E[X])는 변수 X의 편차를 의미하며, (Y - E[Y])는 변수 Y의 편차를 의미합니다. 공분산은 X와 Y의 편차들의 곱의 평균으로 계산됩니다.

    - 공분산의 값이 양수인 경우, 두 변수는 양의 상관 관계를 가지고 있음을 나타냅니다. 즉, 하나의 변수가 증가할 때 다른 변수도 증가하는 경향이 있습니다. 공분산의 값이 음수인 경우, 두 변수는 음의 상관 관계를 가지고 있음을 나타냅니다. 하나의 변수가 증가할 때 다른 변수는 감소하는 경향이 있습니다. 공분산의 값이 0인 경우, 두 변수는 서로 독립적이거나 선형 관계가 없음을 나타냅니다.

    - 상관계수(Correlation Coefficient):

        - 상관계수는 공분산을 각 변수의 표준 편차로 나누어 정규화한 값으로, -1에서 1 사이의 값을 가집니다. 두 변수 X와 Y의 상관계수는 다음과 같이 정의됩니다:

            - ρ(X, Y) = Cov(X, Y) / (σ(X) * σ(Y))

            - 여기서 ρ는 상관계수를 나타내며, Cov는 공분산, σ는 표준 편차를 의미합니다. 상관계수는 공분산을 각 변수의 변동성으로 나눈 것이므로, 단위에 의존하지 않고 상대적인 관계를 나타냅니다.

    - 상관계수의 값이 1에 가까울수록 두 변수는 양의 선형 상관 관계를 가지고 있음을 나타냅니다. 상관계수의 값이 -1에 가까울수록 두 변수는 음의 선형 상관 관계를 가지고 있음을 나타냅니다. 상관계수의 값이 0에 가까울수록 두 변수는 선형적인 상관 관계가 없거나 약한 상관 관계를 가지고 있음을 나타냅니다.

    - 공분산과 상관계수는 두 변수 간의 관계를 측정하는 데 사용되며, 데이터 분석, 통계적 추론, 상관 분석 등에서 중요한 개념으로 활용됩니다.


- 신뢰 구간의 정의는 무엇인가요?

    - 신뢰 구간(Confidence Interval)은 통계적 추정에서 사용되는 개념으로, 표본 데이터를 기반으로 모집단의 특성을 추정하는 구간을 의미합니다. 신뢰 구간은 추정값 주변에 존재할 것으로 예상되는 모수의 범위를 나타냅니다.

    - 일반적으로, 신뢰 구간은 다음과 같이 표현됩니다: [추정값 - 오차, 추정값 + 오차]

    - 여기서 추정값은 표본 데이터를 기반으로 계산된 모집단의 특성을 나타내는 값입니다. 오차는 신뢰 수준과 표본의 변동성에 따라 결정되는 범위입니다. 신뢰 수준은 추정의 정확성을 나타내며, 일반적으로 95% 또는 99%와 같이 표현됩니다.

    - 신뢰 구간을 구하는 과정은 일반적으로 표본의 평균, 분산 또는 비율에 대한 신뢰 구간을 계산하는 것입니다. 표본의 크기, 모집단의 분포, 신뢰 수준 등에 따라 신뢰 구간의 폭이 달라질 수 있습니다. 일반적으로 표본 크기가 크고 변동성이 낮을수록 신뢰 구간의 폭은 좁아집니다.

    - 신뢰 구간은 추정값에 대한 불확실성을 고려하면서도 모집단의 특성에 대한 추정을 제공합니다. 신뢰 구간은 통계적 추정의 신뢰성을 평가하고 모집단에 대한 정보를 제공하는 데 사용됩니다. 신뢰 구간을 사용하면 표본을 통해 얻은 추정값이 얼마나 신뢰할 수 있는지를 평가할 수 있습니다.


- p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요?

    - p-value는 통계적 가설 검정에서 사용되는 개념으로, 주어진 데이터가 특정 가설에 얼마나 일치하는지를 나타내는 지표입니다. p-value는 관찰된 데이터가 우연히 발생한 것인지, 아니면 진짜로 특이한 현상인지를 판단하는 데 사용됩니다.

    - 간단히 말하면, p-value는 "귀무 가설"이라 불리는 기준을 통해 데이터의 통계적인 유의성을 평가하는 값입니다. 귀무 가설은 일반적으로 "두 그룹 간에 차이가 없다" 또는 "어떤 인과 관계가 존재하지 않는다"와 같이 특정 가정을 나타냅니다.

    - p-value의 값은 0과 1 사이의 범위에 있으며, 일반적으로 0.05 또는 0.01과 같이 사전에 설정된 임계값과 비교됩니다. p-value가 임계값보다 작을 경우, 우리는 귀무 가설을 기각하고 "대립 가설"을 채택할 수 있습니다. 이는 데이터가 귀무 가설과 일치하지 않고, 특이한 현상을 나타내는 것으로 해석됩니다. 그러나 p-value가 임계값보다 크다면, 우리는 귀무 가설을 기각할 증거가 충분하지 않다고 판단하고, 통계적으로 유의하지 않은 결과로 간주할 수 있습니다.

    - p-value는 통계적인 가설 검정에서 중요한 개념으로 사용되며, 데이터 분석, 연구 결과의 신뢰성 평가, 의학적인 실험 결과의 해석 등 다양한 분야에서 활용됩니다. 이를 통해 우리는 데이터의 유의성을 평가하고, 의사 결정을 내리는 데 도움을 줄 수 있습니다.

- R square의 의미는 무엇인가요?

    - R 제곱(R-squared)은 회귀 분석에서 사용되는 통계적인 지표로, 종속 변수의 변동 중 독립 변수들이 설명하는 비율을 나타냅니다. R 제곱은 종속 변수의 변동을 독립 변수들이 얼마나 잘 설명하는지를 평가하는데 사용됩니다.

    - R 제곱은 0부터 1까지의 값을 가지며, 높은 값일수록 모델이 데이터를 잘 설명한다는 것을 의미합니다. R 제곱이 1에 가까울수록 독립 변수들이 종속 변수의 변동을 거의 설명하는 것이며, 0에 가까울수록 독립 변수들이 종속 변수의 변동을 설명하지 못하는 것입니다.

    - 수식적으로 R 제곱은 다음과 같이 정의됩니다:

        - R^2 = 1 - (SSR / SST)

        - 여기서 SSR은 잔차의 제곱합(Residual Sum of Squares)이며, SST는 총 변동의 제곱합(Total Sum of Squares)입니다. SSR은 모델로부터 예측된 값과 실제 값 간의 차이를 제곱하여 합산한 값입니다. SST는 실제 종속 변수 값과 평균 값 간의 차이를 제곱하여 합산한 값입니다.

    - R 제곱은 종속 변수의 변동 중 모델로 설명할 수 있는 변동의 비율을 나타내므로, 모델의 설명력을 평가하는데 사용됩니다. 그러나 R 제곱은 독립 변수의 개수나 모델의 복잡성에 영향을 받기 때문에 다른 평가 지표와 함께 사용되어야 합니다. 예를 들어, 조정된 R 제곱(Adjusted R-squared)은 독립 변수의 개수와 표본 크기를 고려하여 모델의 설명력을 보정한 지표입니다.

    - R 제곱은 회귀 분석에서 모델의 적합도를 평가하고 변수의 중요성을 판단하는 데 사용됩니다. 그러나 R 제곱 값 자체만으로 모델의 유효성을 결정하는 것은 적절하지 않습니다. 다른 평가 지표와 함께 고려하여 모델을 평가하는 것이 바람직합니다.


- 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?

    - 평균과 중앙값은 데이터의 중심 경향성을 나타내는 지표로, 각각 다른 상황에서 사용됩니다. 선택해야 할 적절한 케이스는 데이터의 분포와 관련이 있습니다.

    - 평균(mean):

        - 평균은 데이터의 총합을 데이터의 개수로 나눈 값으로, 일반적으로 데이터의 중심 경향성을 나타내는 가장 일반적인 지표입니다. 평균은 데이터의 모든 값을 고려하며, 이상치(outlier)에 영향을 받을 수 있습니다. 따라서 데이터가 정규 분포와 비슷한 형태를 가지고 이상치가 없는 경우에 적합합니다. 평균은 대부분의 수치 계산이나 통계 분석에서 사용되며, 데이터의 총합을 균등하게 나눈 값으로 해석됩니다.

    - 중앙값(median):

        - 중앙값은 데이터를 크기순으로 정렬했을 때 가장 가운데 위치한 값입니다. 중앙값은 이상치의 영향을 받지 않는 장점이 있으며, 데이터의 분포가 비대칭적이거나 이상치가 있는 경우에 유용합니다. 데이터의 크기에 민감하게 반응하기 때문에 극단적인 값에 큰 영향을 받지 않습니다. 중앙값은 데이터의 중앙에 위치한 값으로 해석됩니다.

    - 어떤 지표를 사용해야 하는지는 데이터의 특성과 분석 목적에 따라 결정됩니다. 일반적으로 데이터의 분포와 이상치의 유무, 대상 변수의 성질을 고려하여 평균 또는 중앙값 중 하나를 선택합니다. 또한, 평균과 중앙값을 함께 사용하여 데이터의 중심 경향성을 더욱 정확하게 파악하는 경우도 있습니다.


- 중심극한정리는 왜 유용한걸까요?

    - 중심극한정리(Central Limit Theorem)는 통계학에서 매우 중요하고 유용한 개념입니다. 중심극한정리는 다음과 같이 설명됩니다:

    - "독립적인 랜덤 변수들의 합 또는 평균은 표본 크기가 충분히 크다면, 근사적으로 정규 분포를 따른다."

    - 중심극한정리는 다음과 같은 이점을 가지고 있습니다:

        - 정규 분포 근사: 중심극한정리는 많은 독립적인 랜덤 변수들을 합하거나 평균한 결과가 정규 분포에 근사적으로 따른다는 사실을 제공합니다. 이는 많은 통계적 기법과 추론 방법에서 중요한 가정이며, 정규 분포에 기반한 분석을 수행할 수 있게 해줍니다.

        - 표본 크기의 중요성 감소: 중심극한정리에 따르면, 표본 크기가 충분히 크다면 표본의 분포는 모집단의 분포와 더 비슷해집니다. 이는 모집단에 대한 정보가 제한적인 경우에도 표본으로부터 통계적인 추론을 수행할 수 있음을 의미합니다.

        - 추론의 유효성: 중심극한정리는 통계적 추론에서 유용합니다. 예를 들어, 표본의 평균을 사용하여 모집단의 평균을 추정하는 경우, 중심극한정리에 따라 표본 평균은 대체로 정규 분포를 따르게 됩니다. 이를 통해 신뢰 구간을 계산하거나 가설 검정을 수행하는 등의 통계적 추론을 수행할 수 있습니다.

    - 중심극한정리는 통계학의 기초 원리로써 널리 사용되며, 데이터 분석, 추론, 예측 등 다양한 통계적 문제를 해결하는 데 활용됩니다. 중심극한정리의 적용은 통계학자와 데이터 과학자에게 데이터 분석에서 신뢰할 수 있는 추론을 수행할 수 있는 기반을 제공합니다.


- 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.

    - 엔트로피(Entropy)는 정보 이론(Information Theory)에서 사용되는 개념으로, 어떤 확률 분포의 불확실성 또는 정보의 무질서한 정도를 나타냅니다. 엔트로피는 확률 분포의 다양성을 측정하는 지표로서, 정보의 예측 가능성이 낮을수록 엔트로피가 높아지고, 예측 가능성이 높을수록 엔트로피가 낮아집니다.

    - 정확한 엔트로피의 계산은 해당 확률 분포에서의 모든 가능한 사건들에 대한 정보를 고려하여 수행됩니다. 다른 개념들과 마찬가지로 엔트로피도 정보의 단위로서 표현됩니다. 일반적으로 엔트로피는 비트(bit) 단위로 측정되지만, 자연로그를 사용한 네트(nat) 단위로 표현되기도 합니다.

    - 수학적으로 엔트로피는 다음과 같이 정의됩니다:

    - H(X) = - Σ(p(x) * log₂(p(x)))

    - 여기서 H(X)는 확률 변수 X의 엔트로피를 나타내며, p(x)는 X가 어떤 값 x를 가질 확률을 나타냅니다. 위 식은 모든 가능한 값 x에 대해 확률 p(x)를 곱한 후, 이를 로그 함수에 넣고 음수로 변환한 후 합산하는 것을 의미합니다.

    - Information Gain(정보 획득량)은 엔트로피의 변화량을 나타내는 개념으로, 머신 러닝에서 주로 사용됩니다. Information Gain은 어떤 속성을 기준으로 데이터를 분할했을 때, 분할 전후의 엔트로피 차이를 의미합니다. Information Gain이 높을수록 속성을 사용하여 데이터를 분할하는 것이 정보를 가장 효과적으로 얻을 수 있는 방법임을 나타냅니다. 따라서, Decision Tree와 같은 알고리즘에서 속성의 중요도를 평가하고 선택하는 데 사용됩니다.

    - 정리하자면, 엔트로피는 확률 분포의 불확실성을 측정하는 지표이며, Information Gain은 엔트로피의 변화량으로서 속성의 중요성을 평가하는 지표입니다. 이러한 개념들은 정보 이론과 머신 러닝에서 데이터 분석, 패턴 인식, 결정 방법 등 다양한 분야에서 활용되고 있습니다.


- 어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?

    - 모수적 방법론과 비모수적 방법론은 통계 분석에서 데이터를 모델링하고 추론하는 데 사용되는 두 가지 주요한 접근 방법입니다. 각각의 방법론은 다른 데이터 특성과 추론 목적에 따라 선택됩니다.

    - 모수적 방법론:

        - 모수적 방법론은 데이터를 특정한 확률 분포에 맞추어 모델링하는 접근 방법입니다. 이 방법론은 데이터의 확률 분포에 대한 가정을 설정하고, 해당 가정을 기반으로 모델의 모수(parameter)를 추정합니다. 가장 일반적으로 사용되는 모수적 방법론은 평균, 분산 등의 모수를 추정하는데 사용되는 정규 분포, 베르누이 분포, 포아송 분포 등입니다. 모수적 방법론은 추정된 모델을 사용하여 데이터에 대한 예측, 가설 검정, 신뢰 구간 등을 수행할 수 있습니다. 모수적 방법론의 장점은 모델이 상대적으로 간결하고 해석하기 쉽다는 것입니다. 또한, 데이터의 크기가 작을 때에도 효과적인 추론을 수행할 수 있습니다. 하지만, 모델의 가정이 실제 데이터와 일치하지 않을 경우, 추론 결과가 왜곡될 수 있습니다.

    - 비모수적 방법론:

        - 비모수적 방법론은 데이터에 대한 분포에 대한 가정을 하지 않고, 데이터의 순위 또는 순서에 의존하여 추론하는 접근 방법입니다. 비모수적 방법론은 데이터의 분포 형태나 모수에 대한 가정이 없기 때문에, 더 유연한 추론이 가능합니다. 대표적인 비모수적 방법론에는 부트스트래핑(bootstrapping), 커널 밀도 추정(kernel density estimation), 랭크 테스트(rank test) 등이 있습니다. 비모수적 방법론은 데이터의 분포를 자유롭게 모델링하고, 데이터에 대한 정확한 분석을 수행할 수 있습니다. 하지만, 데이터의 크기가 커질수록 계산적으로 많은 리소스가 필요하고, 모수적 방법론에 비해 해석이 어려울 수 있습니다.

    - 따라서, 모수적 방법론은 데이터의 분포에 대한 가정이 타당하고, 모델의 해석이 중요한 경우에 적합합니다. 비모수적 방법론은 데이터의 분포에 대한 가정을 할 수 없거나, 자유로운 모델링이 필요한 경우에 유용합니다. 선택은 데이터의 특성과 분석 목적에 따라 달라질 수 있으며, 종종 두 가지 방법론을 함께 사용하여 결과를 보완하기도 합니다.


- “likelihood”와 “probability”의 차이는 무엇일까요?

    - "Likelihood"와 "probability"는 통계학에서 사용되는 두 가지 관련된 개념이지만, 약간의 차이가 있습니다.

    - 확률(Probability):

        - 확률은 사건(event)이 발생할 가능성을 나타냅니다. 확률은 사전에 정의된 확률 분포를 기반으로 계산됩니다. 일반적으로, 확률은 주어진 사건이 발생할 확률을 표현하며, 0과 1 사이의 값을 가집니다. 예를 들어, 동전 던지기에서 앞면이 나올 확률은 0.5로 표현됩니다.

    - 우도(Likelihood):

        - 우도는 주어진 관측값(데이터)이 특정한 모델 또는 가설에 대해 얼마나 "적합한"지를 나타냅니다. 우도는 모델의 모수(parameter)를 고려하여 계산됩니다. 우도는 모델의 파라미터 값을 고정하고 데이터를 고려하여 계산되는 조건부 확률입니다. 일반적으로, 주어진 데이터를 바탕으로 모델의 파라미터 값을 추정하기 위해 우도를 최대화하는 방향으로 모델을 조정하는 최대 우도 추정(Maximum Likelihood Estimation, MLE) 등을 사용합니다.

    - 간단히 말해, 확률은 사건이 발생할 가능성을 나타내는 반면, 우도는 주어진 데이터가 특정한 모델 또는 가설에 얼마나 "적합한지"를 나타냅니다. 이 두 개념은 확률론과 통계학에서 다른 의미와 용도를 가지고 있으며, 데이터 분석과 추론에서 각각 중요한 역할을 합니다.


- 통계에서 사용되는 bootstrap의 의미는 무엇인가요.

    - Bootstrap은 통계 분석에서 데이터로부터 표본을 반복적으로 추출하여 통계량을 추정하는 방법입니다. Bootstrap은 비모수적 방법론 중 하나로, 표본의 분포나 모수에 대한 가정을 하지 않고 데이터 자체를 이용하여 신뢰구간을 추정하거나 통계적 가설 검정을 수행하는 데 사용됩니다.

    - Bootstrap은 다음과 같은 과정으로 진행됩니다:

        - 원래의 표본 데이터에서 중복을 허용하여 표본을 무작위로 추출합니다. 이를 부트스트랩 표본(bootstrap sample)이라고 합니다. 부트스트랩 표본의 크기는 원래 데이터의 크기와 동일하게 설정됩니다.

        - 부트스트랩 표본을 사용하여 통계량을 계산합니다. 예를 들어, 평균, 중앙값, 분산 등의 통계량을 계산할 수 있습니다.

        - 위 과정을 여러 번 반복하여 여러 개의 부트스트랩 표본과 그에 대응하는 통계량을 얻습니다. 일반적으로 수백 또는 수천 번의 반복을 수행합니다.

        - 부트스트랩 표본에서 얻은 통계량의 분포를 통해 원래의 표본 데이터에 대한 신뢰구간을 추정하거나 가설 검정을 수행할 수 있습니다. 이를 통해 원래 데이터에 대한 통계적인 정보를 얻을 수 있습니다.

    - Bootstrap은 모수적 방법론의 가정을 필요로하지 않으며, 데이터에 대한 자유로운 분포 추정과 추론이 가능합니다. 따라서, 작은 크기의 데이터나 비정규 분포를 가진 데이터에 대해서도 신뢰할 수 있는 추정을 수행할 수 있습니다. 또한, 부트스트랩은 신뢰구간과 가설 검정에 널리 사용되며, 통계적인 추론을 보다 견고하게 만들어줍니다.


- 모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?

    - 모수가 매우 적은 케이스에서 예측 모델을 수립하는 것은 도전적일 수 있지만, 몇 가지 방법을 고려할 수 있습니다. 다음은 모수가 적은 케이스에서 예측 모델을 수립하는 몇 가지 일반적인 방법입니다:

        - 단순한 모델 사용: 모수가 적은 경우에는 복잡한 모델보다 단순한 모델을 고려하는 것이 좋을 수 있습니다. 예를 들어, 선형 회귀 모델이나 로지스틱 회귀 모델과 같이 매개 변수가 적은 모델을 사용할 수 있습니다. 단순한 모델은 해석이 쉽고 과적합의 위험이 적은 편이기 때문에 모델의 일반화 성능을 향상시킬 수 있습니다.

        - 변수 선택: 변수 선택은 모델링에서 모수가 적은 경우 특히 중요합니다. 변수 선택을 통해 중요한 변수만을 고려하여 모델을 구축할 수 있습니다. 변수 선택 방법으로는 정보 이득, 변수 중요도, LASSO(L1 regularization)와 같은 기법을 활용할 수 있습니다.

        - 교차 검증(Cross-validation): 데이터가 제한적인 경우, 교차 검증은 모델의 일반화 성능을 평가하는 데 도움이 될 수 있습니다. 예를 들어, k-fold 교차 검증을 수행하여 데이터를 k개의 서로 다른 부분 집합으로 나누고, 각각의 부분 집합을 순서대로 검증에 사용하여 모델의 성능을 평가합니다.

        - 정규화(Regularization): 정규화는 모델의 복잡성을 제어하여 과적합을 방지하는 기법입니다. 정규화 기법으로는 Ridge regression, LASSO, Elastic Net 등이 있습니다. 정규화는 매개 변수의 수를 제한하고, 중요하지 않은 매개 변수의 영향을 줄여 예측 성능을 향상시킬 수 있습니다.

        - 도메인 지식 활용: 모수가 적은 경우에는 해당 도메인에 대한 지식을 적극적으로 활용하는 것이 유용할 수 있습니다. 도메인 지식을 활용하여 모델을 구축하고 변수를 선택하거나 특징을 추출하는 데 도움을 받을 수 있습니다.

    - 이러한 방법들을 조합하여 모델을 수립하면 모수가 적은 케이스에서도 상당한 예측 성능을 얻을 수 있습니다. 그러나 데이터의 특성과 문제의 복잡성에 따라 적합한 방법을 선택하는 것이 중요합니다.


- 베이지안과 프리퀀티스트 간의 입장차이를 설명해주실 수 있나요?

    - 프리퀀티스트와 베이지안은 통계적 추론에 대한 다른 접근 방식을 가지고 있습니다. 각각의 입장을 설명해드리겠습니다:

    - 프리퀀티스트 (Frequentist):

        - 프리퀀티스트 접근 방식은 빈도주의적인 관점을 갖고 있습니다. 이 접근 방식에서는 모델의 매개 변수(parameter)들은 고정된 값으로 가정되며, 데이터는 랜덤 샘플링의 결과로 간주됩니다. 프리퀀티스트는 표본의 분포에 대한 확률을 추론하고, 모델의 매개 변수 값을 추정하기 위해 최대 우도 추정(Maximum Likelihood Estimation, MLE)과 같은 방법을 사용합니다. 또한, 가설 검정과 신뢰구간을 통해 통계적 추론을 수행합니다.

        - 프리퀀티스트 접근 방식에서는 사전에 정의된 확률 분포를 가정하거나, 사전 지식을 고려하지 않습니다. 데이터를 통해 모델의 매개 변수를 추정하고, 모델의 성능을 평가하며, 추론을 수행합니다. 프리퀀티스트는 반복 가능성(repeatability)과 빈도주의적인 통계적 추론을 강조합니다.

    - 베이지안 (Bayesian):

        - 베이지안 접근 방식은 베이즈 정리를 기반으로 합니다. 이 접근 방식에서는 모델의 매개 변수 자체를 확률 변수로 취급합니다. 베이지안은 사전 지식, 주관적인 믿음, 경험 등을 바탕으로 사전 확률(prior probability)을 설정하고, 이후 데이터를 통해 사후 확률(posterior probability)을 업데이트합니다.

        - 베이지안 접근 방식에서는 모델의 불확실성을 확률적으로 모델링하며, 사전 분포와 데이터를 결합하여 사후 분포를 추정합니다. 이를 통해 모델의 매개 변수에 대한 불확실성을 추론할 수 있습니다. 베이지안은 모델의 불확실성을 명시적으로 다룰 수 있으며, 사전 지식과 데이터를 조합하여 추론을 수행하는데 강점을 가지고 있습니다.

        - 베이지안 접근 방식은 확률적인 개념과 주관성을 강조하며, 개별 사례에 대한 개인적인 믿음과 사전 지식의 영향을 받습니다. 반면에 프리퀀티스트 접근 방식은 빈도적인 개념과 반복성을 강조하며, 주어진 데이터로부터 일반화하고 통계적인 추론을 수행합니다.

    - 요약하자면, 프리퀀티스트와 베이지안은 통계적 추론에 대한 개념과 방법론에서 차이를 가지고 있으며, 확률의 해석과 불확실성의 처리에 대한 접근 방식에서 차이가 있습니다.


- 검정력(statistical power)은 무엇일까요?

    - 검정력(statistical power)은 통계적 가설 검정에서 유의 수준(significance level)과 함께 중요한 개념입니다. 검정력은 통계적으로 유의한 효과를 감지할 수 있는 능력을 의미합니다. 즉, 검정력은 진실이나 효과가 존재할 때 해당 효과를 식별할 수 있는 확률입니다.

    - 검정력은 주로 다음과 같은 요소에 의해 결정됩니다:

        - 효과 크기 (Effect Size): 검정력은 실제로 존재하는 효과의 크기에 영향을 받습니다. 효과 크기가 크면 검정력이 높아지고, 작으면 검정력이 낮아집니다.

        - 표본 크기 (Sample Size): 표본의 크기가 증가하면 검정력이 높아집니다. 더 많은 데이터를 사용하면 작은 효과도 통계적으로 유의미하게 감지할 수 있습니다.

        - 유의 수준 (Significance Level): 유의 수준이 낮을수록 (예: 0.05보다 낮을수록) 검정력이 낮아집니다. 보다 엄격한 기준으로 유의성을 판단하기 때문에 효과를 검출하기 어려워집니다.

        - 통계적 분석 방법: 사용하는 통계적 분석 방법에 따라 검정력이 달라질 수 있습니다. 특정 상황에 더 적합한 통계적 방법을 선택하면 검정력을 향상시킬 수 있습니다.

    - 검정력은 통계적 가설 검정에서 중요한 개념입니다. 만약 검정력이 낮다면, 실제로 존재하는 효과를 잘못으로 감지하지 못할 수 있습니다. 따라서, 충분한 검정력을 갖는 검정을 수행하여 유의한 결과를 얻을 수 있도록 주의해야 합니다.


- missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?

    - Missing value가 있는 경우에는 결측값을 채우는 것이 일반적으로 권장됩니다. 이는 다음과 같은 이유로 인해 중요합니다:

        - 통계적 효율성: 결측값을 채움으로써 데이터의 통계적 효율성을 향상시킬 수 있습니다. 결측값이 포함된 데이터를 사용하면 표본 크기가 줄어들어 분석 결과의 신뢰도가 낮아질 수 있습니다. 결측값을 적절히 채움으로써 표본의 크기를 최대한 활용하고, 불필요한 정보의 손실을 최소화할 수 있습니다.

        - 편향성 감소: 결측값이 무작위로 발생하지 않는 경우, 결측값이 있는 변수에 편향성이 발생할 수 있습니다. 이는 데이터 분석 결과에 왜곡을 초래할 수 있습니다. 결측값을 적절히 채움으로써 이러한 편향성을 줄이고, 데이터의 정확성과 신뢰성을 향상시킬 수 있습니다.

        - 완결성 유지: 결측값을 채움으로써 데이터의 완결성을 유지할 수 있습니다. 결측값이 있는 변수를 분석에서 제외하면 해당 변수에 대한 정보를 완전히 무시하게 되는데, 이는 모델의 예측력을 저하시킬 수 있습니다. 결측값을 채움으로써 데이터의 완결성을 유지하고, 보다 포괄적인 분석을 수행할 수 있습니다.

    - 결측값을 채우는 방법은 다양하며, 데이터의 특성과 결측값의 패턴에 따라 선택됩니다. 일반적으로는 결측값 대체(Imputation) 기법이 사용되며, 대체 방법으로는 평균, 중앙값, 최빈값, 회귀 예측, 다중 대체 등이 있습니다. 결측값을 채울 때는 가능한 한 주의를 기울여야 하며, 결측값을 채우는 방법과 그 결과가 분석에 어떤 영향을 미칠 수 있는지 고려해야 합니다.


- 아웃라이어의 판단하는 기준은 무엇인가요?

    - 아웃라이어(Outlier)는 데이터 집합에서 일반적인 패턴과 동떨어진 극단적인 값을 가지는 관측치를 말합니다. 아웃라이어는 주로 다음과 같은 기준을 사용하여 판단할 수 있습니다:

        - 통계적 기준: 통계적으로 아웃라이어를 판단하기 위해 일반적으로 평균과 표준편차를 사용합니다. 표준편차의 여러 배 이상 떨어진 값은 아웃라이어로 간주될 수 있습니다. 또는 z-점수를 계산하여 특정 임계값을 넘는 경우 아웃라이어로 판단할 수도 있습니다.

        - 상자 그림(Box plot): 상자 그림은 데이터의 분포와 이상치를 시각적으로 확인하는데 사용됩니다. 상자 그림은 데이터의 하한, 상한, 중앙값, 이상치 등을 보여주어 아웃라이어를 쉽게 식별할 수 있게 해줍니다.

        - 도메인 지식과 주관적 판단: 도메인 지식과 주관적 판단은 데이터 분석가가 특정 문제 영역에 대한 전문성을 바탕으로 아웃라이어를 판단하는 데 도움을 줄 수 있습니다. 특정 값이 현실적으로 가능하지 않거나 실수로 발생한 오류인 경우 아웃라이어로 판단될 수 있습니다.

    - 아웃라이어를 판단하는 기준은 데이터의 특성과 분석 목적에 따라 달라질 수 있습니다. 아웃라이어의 식별은 데이터의 왜곡을 방지하고 분석 결과의 신뢰성을 향상시키는 데 도움을 줄 수 있습니다. 그러나 아웃라이어를 판단할 때는 주의가 필요하며, 아웃라이어의 원인을 파악하고 처리하는 데 있어서 도메인 지식과 전문성을 적극적으로 활용해야 합니다.


- 필요한 표본의 크기를 어떻게 계산합니까?

    - 표본의 크기를 계산하는 방법은 분석하려는 문제의 특성과 목적에 따라 달라집니다. 일반적으로 표본의 크기를 결정하기 위해서는 다음과 같은 요소를 고려해야 합니다:

        - 효과 크기 (Effect Size): 분석하려는 변수 또는 처리 간의 효과 크기를 예측하거나 기대하는 것이 중요합니다. 효과 크기가 클수록 더 많은 표본이 필요할 수 있습니다.

        - 유의 수준 (Significance Level): 유의 수준은 통계적 가설 검정에서 사용되는 기준으로, 주로 0.05 또는 0.01이 사용됩니다. 유의 수준이 낮을수록 더 많은 표본이 필요할 수 있습니다.

        - 검정력 (Statistical Power): 검정력은 표본의 크기와 효과 크기, 유의 수준 사이의 관계를 나타내는 지표입니다. 높은 검정력을 원한다면 더 많은 표본이 필요합니다.

        - 분석 방법: 분석에 사용되는 통계적인 방법에 따라 필요한 표본의 크기가 달라질 수 있습니다. 예를 들어, 회귀 분석이나 t-검정과 같은 분석에서는 더 많은 표본이 필요할 수 있습니다.

        - 자원 제약: 표본 크기는 연구나 분석에 사용 가능한 자원과 예산에 의해 제한될 수 있습니다. 따라서, 자원의 가용성을 고려하여 표본 크기를 결정해야 합니다.

    - 표본 크기를 계산하기 위해서는 각각의 요소를 고려하여 통계적인 계산이나 시뮬레이션 등을 수행할 수 있습니다. 일반적으로 표본 크기를 계산하는 방법은 해당 분석 방법에 대한 통계적인 패키지나 소프트웨어에서 제공하는 기능을 활용하거나, 이전 연구나 문헌에서 유사한 분석을 수행한 결과를 참고하는 방법 등이 있습니다. 표본 크기를 계산하는 방법은 분석의 목적과 문제에 따라 다르므로, 해당 분야의 전문가와 상의하거나 통계적인 컨설턴트의 도움을 받는 것이 좋습니다.


- Bias를 통제하는 방법은 무엇입니까?

    - Bias를 통제하기 위해 다음과 같은 방법들을 사용할 수 있습니다:

        - 표본 선택: 적절한 표본 선택은 bias를 통제하는 데 중요합니다. 표본이 대상 모집단을 대표하고 있는지 확인하고, 표본 선택 과정에서 편향을 피하기 위해 무작위 또는 계획적인 표본 추출 방법을 사용해야 합니다.

        - 변수 선택: 분석에 사용되는 변수들을 신중하게 선택하여 bias를 통제할 수 있습니다. 불필요한 변수를 제거하고, 주요 변수들과 관련이 있는 보조 변수들을 포함시켜 bias를 줄일 수 있습니다.

        - 변수 측정: 변수를 정확하게 측정하는 것이 중요합니다. 정확한 측정 도구를 사용하고, 일관된 방식으로 변수를 측정함으로써 bias를 피할 수 있습니다.

        - 표본 크기: 충분한 표본 크기를 사용하여 bias를 통제할 수 있습니다. 표본 크기가 작을 경우, 샘플링 변동이 커져 bias가 발생할 가능성이 높아집니다. 큰 표본 크기를 사용하면 bias를 줄일 수 있습니다.

        - 조절 변수 (Control variables): 다양한 요인으로 인한 bias를 통제하기 위해 조절 변수를 사용할 수 있습니다. 조절 변수는 주요 변수와 결과 사이의 관계에서 발생하는 외부 요인을 통제하는 데 사용됩니다.

        - 분석 방법: 적절한 분석 방법을 선택하여 bias를 통제할 수 있습니다. 예를 들어, 회귀 분석에서 다중공선성을 피하기 위해 변수 선택이나 변수 변환을 수행하거나, 일반화 선형 모델에서 규제를 사용하여 bias를 줄일 수 있습니다.

        - 외생성 가정: 분석 모델의 외생성 가정을 만족시키는 것이 중요합니다. 외생성 가정이란 분석 모델의 오류 항이 관심 변수와 독립적이라는 가정입니다. 이를 위해 독립 변수들과 오류 항 간의 상관 관계를 고려하고, 오류 항에 영향을 미치는 외부 요인을 제어하는 것이 중요합니다.

    - 위의 방법들은 bias를 통제하기 위해 고려해야 할 중요한 요소들입니다. 분석의 목적과 문제에 따라 적절한 방법들을 선택하여 bias를 최소화하고 신뢰성 있는 결과를 얻을 수 있습니다.


- 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.

    - 로그 함수는 다양한 분야에서 유용하게 사용될 수 있습니다. 몇 가지 대표적인 사례를 들어 설명해보겠습니다:

        - 데이터 스케일 조정: 로그 함수는 데이터의 스케일을 조정하는 데 사용될 수 있습니다. 특히, 데이터가 지나치게 큰 값을 가지는 경우 로그 변환을 통해 데이터의 분포를 더 정규 분포에 가깝게 만들 수 있습니다. 이는 통계 분석이나 머신러닝 모델 학습에 도움이 될 수 있습니다.

        - 지수적 관계 모델링: 많은 자연 현상이 지수적 관계를 따르는 경우가 많습니다. 이러한 경우에 로그 함수를 사용하여 지수적 관계를 선형적으로 모델링할 수 있습니다. 예를 들어, 경제학에서 소득과 소비, 인구 증가와 자원 소비 등은 지수적 관계를 가질 수 있으며, 로그 변환을 통해 이러한 관계를 선형적으로 모델링할 수 있습니다.

        - 정보 이론: 로그 함수는 정보 이론에서 중요한 개념인 엔트로피와 관련이 있습니다. 엔트로피는 정보의 불확실성을 측정하는 지표로 사용되며, 로그 함수를 통해 계산됩니다. 엔트로피는 확률 분포의 불확실성을 나타내는데 유용하게 활용됩니다.

        - 금융 및 경제학: 금융 및 경제학에서 로그 함수는 수익률의 변동성을 측정하는 데 사용됩니다. 로그 수익률은 주식 시장에서 일반적으로 사용되며, 로그 변환을 통해 수익률의 통계적 특성을 조사하고 예측하는 데 유용합니다.

    - 이 외에도 로그 함수는 다양한 분야에서 데이터 처리, 정보 표현, 확률 계산 등에 유용하게 활용될 수 있습니다. 로그 함수는 데이터의 특성과 분석 목적에 따라 유연하게 사용될 수 있는 강력한 도구입니다.


- 베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포에 대해 설명해주세요. 그리고 분포 간의 연관성도 설명해주세요.

    - 분포에 대한 설명과 분포 간의 연관성에 대해 간단히 설명해드리겠습니다:

        - 베르누이 분포 (Bernoulli Distribution):

            - 이항 분포의 특수한 경우로, 단일 베르누이 시행의 결과를 모델링합니다.

            - 예를 들어 동전 던지기에서 앞면(성공)이 나올 확률을 모델링할 수 있습니다.

        - 이항 분포 (Binomial Distribution):

        -    베르누이 시행을 독립적으로 여러 번 수행하는 경우를 모델링합니다.

            - 각 시행에서의 성공 확률과 시행 횟수가 주어질 때, 성공 횟수를 나타냅니다.

            - 동전 던지기에서 앞면이 나올 횟수, 특정 제품의 결함 발생 횟수 등을 모델링할 수 있습니다.

        - 카테고리 분포 (Categorical Distribution):

            - 여러 개의 범주 중 하나를 선택하는 상황을 모델링합니다.

            - 각 범주의 선택 확률이 주어질 때, 특정 범주의 선택을 나타냅니다.

            - 주사위를 던져 나오는 눈의 숫자, 선호하는 제품 카테고리 등을 모델링할 수 있습니다.

        - 다항 분포 (Multinomial Distribution):

            - 독립적인 여러 개의 카테고리 분포를 모델링합니다.

            - 각 카테고리의 선택 확률이 주어질 때, 각 카테고리의 선택 횟수를 나타냅니다.

            - 주사위 여러 개를 동시에 던져 각 눈의 숫자가 나오는 횟수, 여러 제품 카테고리의 판매량 등을 모델링할 수 있습니다.

        - 가우시안 정규 분포 (Gaussian Normal Distribution):

            - 연속형 변수를 모델링하는 가장 일반적인 분포입니다.

            - 평균과 분산을 특징으로 하며, 종모양의 대칭적인 분포입니다.

            - 키, 체중, 성적 등 연속형 변수의 분포를 모델링할 수 있습니다.

        - t 분포 (t-Distribution):

            - 표본의 크기가 작은 경우에 사용되며, 정규 분포의 평균에 대한 검정 등에 활용됩니다.

            - 정규 분포와 유사하지만 꼬리 부분이 더 두껍고 넓은 분포입니다.

        - 카이제곱 분포 (Chi-Square Distribution):

            - 정규 분포를 따르는 모집단에서의 분산 추정이나 독립성 검정 등에 사용됩니다.

            - 자유도 파라미터에 따라 다양한 모양을 가지는 분포입니다.

        - F 분포 (F-Distribution):

            - 분산 비교, 회귀 분석 등에서 사용되며, 두 개 이상의 분산을 비교하는 데에 활용됩니다.

            - 두 개의 카이제곱 분포를 사용하여 구성됩니다.

        - 베타 분포 (Beta Distribution):

            - 0과 1 사이의 값을 모델링하는 데 사용됩니다.

            - 베르누이 분포의 모수인 성공과 실패의 확률을 모델링할 수 있습니다.

        - 감마 분포 (Gamma Distribution):

            - 양수 값을 모델링하는 데 사용됩니다.

            - 포아송 분포의 발생 횟수, 대기 시간 등을 모델링할 수 있습니다.

    - 이러한 분포들은 통계 분석에서 데이터를 모델링하고 가정하는 데 사용됩니다. 분포 간의 연관성은 여러 분포가 서로 관련되어 있거나 파생 관계를 가지고 있을 수 있음을 의미합니다. 예를 들어, t 분포는 정규 분포로부터 유도되며, 카이제곱 분포와 F 분포는 t 분포와 관련이 있습니다. 베타 분포는 카테고리 분포의 일반화된 형태이며, 감마 분포는 베타 분포의 일반화된 형태로 볼 수 있습니다. 이렇게 분포들 간의 관계를 이해하면 통계 분석에서 다양한 분포를 적용하고 문제를 해결하는 데 도움이 됩니다.


- 출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?

    - 이 문제는 조건부 확률을 활용하여 해결할 수 있습니다.

    - A를 "비가 내리는 경우"로 정의하겠습니다.

    - B1, B2, B3를 각각 친구 1, 친구 2, 친구 3이 "그렇습니다. 비가 내리고 있습니다"라고 말한 경우로 정의하겠습니다.

    - 우리가 구하고자 하는 것은 실제로 비가 내릴 때, 친구들이 모두 "그렇습니다. 비가 내리고 있습니다"라고 말한 상황입니다. 이를 표현하면 P(A|B1∩B2∩B3)입니다.

    - 조건부 확률의 정의에 따라 이를 다음과 같이 풀어쓸 수 있습니다:

    - P(A|B1∩B2∩B3) = (P(B1∩B2∩B3|A) * P(A)) / P(B1∩B2∩B3)

    - P(B1∩B2∩B3|A)는 A가 주어졌을 때 친구들이 동시에 "그렇습니다. 비가 내리고 있습니다"라고 말할 확률입니다. 이는 독립 사건이 아니기 때문에 각각의 조건부 확률을 곱해서 계산합니다.

    - P(A)는 실제로 비가 내릴 확률입니다. 문제에서는 주어지지 않았으므로 우리가 가정해야 합니다.

    - P(B1∩B2∩B3)는 친구들이 동시에 "그렇습니다. 비가 내리고 있습니다"라고 말할 확률입니다.

    - 주어진 문제에서는 친구들이 모두 "그렇습니다. 비가 내리고 있습니다"라고 말했으므로 P(B1∩B2∩B3)를 계산해야 합니다. 이를 계산하기 위해서는 친구들이 거짓말을 할 경우와 진실을 말할 경우를 나눠서 생각해야 합니다.

    - 친구들이 거짓말을 할 경우, 모든 친구들이 동시에 거짓말을 하는 확률은 (1/3)^3 = 1/27입니다.
    
    - 친구들이 진실을 말할 경우, 모든 친구들이 동시에 진실을 말하는 확률은 (2/3)^3 = 8/27입니다.

    - 따라서 P(B1∩B2∩B3) = (1/27) * P(A) + (8/27) * (1 - P(A))입니다.

    - 문제에서 주어진 것처럼 각 친구는 2/3의 확률로 진실을 말하고 1/3의 확률로 거짓을 말합니다. 따라서 실제로 비가 내릴 확률인 P(A)는 2/3입니다.

    - 따라서 P(B1∩B2∩B3) = (1/27) * (2/3) + (8/27) * (1 - 2/3) = 2/27 + 16/27 = 18/27 = 2/3입니다.

    - 따라서 실제로 비가 내릴 확률은 2/3입니다.



🤖 Machine Learning

- 알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)

    - Metric은 모델의 성능을 측정하거나 평가하기 위해 사용되는 측정 지표입니다. 다양한 문제에 따라 적합한 metric을 선택하여 모델의 성능을 정량화할 수 있습니다. 여기에는 몇 가지 일반적인 metric을 설명하겠습니다:

        - 평균 제곱근 오차 (RMSE, Root Mean Square Error):

            - 회귀 문제에서 예측 값과 실제 값의 차이를 측정하는 지표입니다.

            - 오차를 제곱한 후 평균을 구하고, 다시 제곱근을 취한 값입니다.

            - 예측 값과 실제 값 사이의 거리를 나타내며, 값이 작을수록 모델의 성능이 좋습니다.

        - 평균 절대 오차 (MAE, Mean Absolute Error):

            - 회귀 문제에서 예측 값과 실제 값의 절대 차이를 측정하는 지표입니다.

            - 오차를 절대값으로 변환한 후 평균을 구합니다.

            - 예측 값과 실제 값 사이의 거리를 나타내며, 값이 작을수록 모델의 성능이 좋습니다.

        - 정확도 (Accuracy):

            - 분류 문제에서 모델의 전체적인 예측 성능을 평가하는 지표입니다.

            - 전체 샘플 중 정확하게 분류된 샘플의 비율을 계산합니다.

            - 다중 클래스 분류에서는 각 클래스별로 정확도를 계산할 수도 있습니다.

        - 정밀도 (Precision):

            - 이진 분류 문제에서 양성으로 예측된 샘플 중 실제로 양성인 샘플의 비율을 계산하는 지표입니다.

            - 거짓 양성(FP)의 수를 줄이는 것을 목표로 합니다.

        - 재현율 (Recall):

            - 이진 분류 문제에서 실제 양성인 샘플 중 양성으로 예측된 샘플의 비율을 계산하는 지표입니다.

            - 거짓 음성(FN)의 수를 줄이는 것을 목표로 합니다.

        - F1 점수 (F1 Score):

            - 정밀도와 재현율의 조화 평균으로 계산되는 지표입니다.

            - 이진 분류에서 정밀도와 재현율을 동시에 고려하여 모델의 성능을 평가합니다.

        - 로그 손실 (Log Loss):

            - 이진 분류 또는 다중 클래스 분류 문제에서 모델의 예측 확률과 실제 레이블 사이의 오차를 측정하는 지표입니다.

            - 낮은 로그 손실 값은 더 좋은 모델의 성능을 의미합니다.

        - 혼동 행렬 (Confusion Matrix):

            - 이진 분류 또는 다중 클래스 분류에서 실제 레이블과 예측 결과를 행렬로 표현한 것입니다.

            - 참 양성(TP), 거짓 양성(FP), 참 음성(TN), 거짓 음성(FN)의 수를 나타내어 모델의 분류 성능을 평가합니다.

    - 이 외에도 다양한 metric이 있으며, 문제의 특성과 목표에 따라 선택되고 해석됩니다. 각 metric은 모델의 성능을 다양한 측면에서 평가하고 비교하는 데 도움을 줍니다.

- 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?

    - 정규화는 데이터를 일정한 범위로 조정하는 과정입니다. 이는 다음과 같은 이유로 필요합니다:

    - 서로 다른 단위와 스케일을 가진 변수들 간의 비교를 용이하게 합니다. 변수 간의 범위 차이가 클 경우, 범위가 큰 변수가 모델 학습에 지배적인 영향을 미칠 수 있습니다. 정규화를 통해 변수들을 동일한 스케일로 조정하여 공정한 비교를 할 수 있습니다.

    - 모델의 수렴 속도를 향상시킵니다. 정규화는 변수들의 값 범위를 축소시키고, 이로 인해 모델 학습 시 수렴 속도가 향상될 수 있습니다.

    - 이상치의 영향을 완화시킵니다. 이상치는 데이터의 분포를 왜곡시킬 수 있으며, 모델의 성능을 저하시킬 수 있습니다. 정규화는 변수를 일정한 범위로 조정하여 이상치의 영향을 최소화합니다.

    - 일반적으로 사용되는 정규화 방법에는 다음과 같은 것들이 있습니다:

    - Min-Max 정규화:

        - 변수 값을 0과 1 사이로 조정합니다.

        - 각 변수의 최솟값을 0, 최댓값을 1로 가정하고, 실제 값들을 이에 매핑합니다.

    - Z-Score 정규화:

        - 변수 값을 평균이 0, 표준편차가 1인 표준 정규분포로 변환합니다.

        - 각 변수의 평균과 표준편차를 계산하여 실제 값들을 이에 매핑합니다.

    - 로그 변환:

        - 변수의 값을 로그 함수를 적용하여 변환합니다.

        - 데이터의 분포가 왜곡되어 있을 때 사용하며, 데이터의 스케일을 조정합니다.

    - 단위 길이로 조정:

        - 다차원 벡터의 크기를 단위 길이로 조정합니다.

        - 벡터의 크기를 1로 만들기 위해 각 변수의 값을 벡터의 크기로 나눕니다.

    - 이외에도 다양한 정규화 방법이 있으며, 데이터의 특성과 모델에 따라 적절한 정규화 방법을 선택하여 사용해야 합니다.


- Local Minima와 Global Minima에 대해 설명해주세요.

    - Local Minima와 Global Minima는 함수의 최솟값을 나타내는 개념입니다.

    - Local Minima (지역 최소값):

        - 함수가 특정 지점에서 최소값을 가지는 경우를 말합니다. 이는 해당 지점에서의 기울기가 0이 되는 지점으로, 그 근방에서는 다른 점보다 작은 값을 가집니다. 그러나 전체 함수에서는 최솟값이 아닐 수 있습니다. 함수가 여러 개의 국소 최소값을 가지는 경우, 그 중 가장 작은 값을 가진 지점을 global minima로 판단할 수 있습니다.

    - Global Minima (전역 최소값):

        - 함수 전체에서 가장 작은 값을 나타내는 지점을 말합니다. 이는 함수의 모든 영역에서 다른 점보다 작은 값을 가지며, 함수의 최적해를 나타냅니다. Global Minima는 함수의 전체적인 형태를 고려하여 판단되며, 문제에 따라 유일하거나 여러 개일 수 있습니다.

    - 함수의 최소값을 찾는 최적화 알고리즘을 사용할 때, Local Minima와 Global Minima를 고려해야 합니다. 만약 알고리즘이 Local Minima에 갇히게 되면, 전역 최소값을 찾지 못할 수 있습니다. 따라서 최적화 알고리즘을 설계할 때, 이러한 지역 최소값에 갇히지 않고 전역 최소값을 찾을 수 있는 방법을 고려해야 합니다. 이를 위해 초기값 설정, 다양한 시작점에서 실행, 그리고 알고리즘의 성격에 따른 변형 등이 사용될 수 있습니다.


- 차원의 저주에 대해 설명해주세요.

    - 차원의 저주(Curse of Dimensionality)는 고차원 공간에서 데이터 분석과 패턴 인식을 어렵게 만드는 현상을 뜻합니다. 고차원 데이터에서 발생하는 몇 가지 문제와 어려움을 설명하겠습니다:

        - 데이터 희소성(Sparsity of Data): 고차원 공간에서 데이터 포인트 간의 거리가 멀어지게 되며, 데이터가 희소해집니다. 데이터가 희소하면 모델 학습이 어려워지고, 적은 데이터로는 신뢰할만한 패턴을 찾기 어렵습니다.

        - 차원의 증가와 필요한 데이터 양: 차원이 증가함에 따라 필요한 데이터의 양도 기하급수적으로 증가합니다. 고차원 공간에서는 적은 양의 데이터로는 모델이 충분히 학습되지 않을 수 있습니다. 이로 인해 고차원 데이터셋에서는 데이터 수집에 대한 더 큰 비용과 시간이 필요하게 됩니다.

        - 차원의 저주와 모델 복잡도: 고차원 데이터에서는 모델이 복잡해지고 과적합(Overfitting)의 위험이 증가합니다. 너무 많은 변수나 특성이 있는 경우, 모델은 잡음에 쉽게 반응할 수 있으며, 실제로는 유용한 패턴을 찾기 어려울 수 있습니다.

        - 거리 측정과 유사도: 고차원 공간에서는 데이터 포인트 간의 거리 측정이 복잡해집니다. 이로 인해 데이터 간의 유사성을 판단하기 어렵고, 클러스터링이나 이상치 탐지와 같은 작업이 어려워집니다.

    - 따라서 차원의 저주는 고차원 데이터의 처리와 분석에 있어서 데이터 희소성, 데이터 양, 모델 복잡도, 거리 측정과 유사도 등 다양한 어려움을 초래합니다. 이를 극복하기 위해서는 변수 선택, 차원 축소, 데이터 전처리 기법 등을 활용하여 데이터의 차원을 줄이고, 모델의 복잡도를 관리하는 등의 전략을 사용할 수 있습니다.


- dimension reduction기법으로 보통 어떤 것들이 있나요?

    - 차원 축소(Dimensionality Reduction)는 고차원 데이터의 특성을 보존하면서 데이터의 차원을 줄이는 기법입니다. 이를 통해 데이터의 복잡성을 낮추고, 시각화, 데이터 압축, 노이즈 제거 등 다양한 목적을 달성할 수 있습니다. 일반적으로 사용되는 차원 축소 기법으로는 다음과 같은 것들이 있습니다:

        - 주성분 분석(Principal Component Analysis, PCA): 가장 일반적으로 사용되는 차원 축소 알고리즘으로, 데이터의 분산을 최대한 보존하면서 주요한 정보를 추출합니다. 고차원 데이터의 변수들 사이의 상관관계를 고려하여 새로운 축으로 데이터를 변환합니다.

        - t-SNE(t-Distributed Stochastic Neighbor Embedding): 고차원 데이터의 시각화를 위해 사용되는 비선형 차원 축소 알고리즘입니다. 데이터 포인트들 간의 유사도를 보존하면서 저차원 공간으로 매핑합니다.

        - LLE(Locally Linear Embedding): 지역적으로 선형 관계가 유지되는 데이터의 저차원 표현을 찾는 알고리즘입니다. 이웃 데이터 포인트 간의 선형 관계를 유지하면서 저차원 임베딩을 수행합니다.

        - 병합 군집(Feature Agglomeration): 특성들을 그룹화하여 차원을 축소하는 방법입니다. 유사한 특성들을 하나의 그룹으로 합치는 과정을 거쳐 차원을 줄입니다.

        - 자동 인코더(Autoencoder): 신경망 기반의 차원 축소 기법으로, 입력과 출력이 같은 구조를 가지는 인코더-디코더 모델을 학습시켜 잠재적인 특성을 추출합니다.

        - 커널 PCA(Kernel PCA): PCA를 커널 트릭을 사용하여 비선형 차원 축소로 확장한 기법입니다. 데이터를 고차원 특징 공간으로 사상한 후 PCA를 수행하여 저차원으로 축소합니다.

    - 이 외에도 다양한 차원 축소 기법이 존재하며, 문제의 특성과 목적에 따라 적절한 기법을 선택해야 합니다. 또한, 차원 축소 기법의 효과를 평가하기 위해 보존된 분산의 비율, 시각화 결과, 분류 또는 회귀 모델의 성능 등을 고려해야 합니다.


- PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?

    - PCA는 차원 축소 기법으로 주로 사용되지만, 동시에 데이터 압축과 노이즈 제거에도 효과적으로 활용될 수 있습니다. 이는 PCA의 작동 원리에 기인합니다.

    - PCA는 데이터의 분산을 최대화하는 주성분을 찾아 데이터를 새로운 축으로 변환하는 과정을 거칩니다. 이때, 분산이 큰 주성분은 데이터의 변동성을 가장 잘 설명하는 요소로서, 데이터를 가장 중요한 특성으로 표현할 수 있습니다. 따라서, 데이터의 차원을 줄이면서도 대부분의 정보를 보존할 수 있게 됩니다.

    - 데이터 압축:

        - PCA는 주성분을 선택하여 데이터를 저차원 공간으로 투영함으로써 데이터의 차원을 줄이는 효과를 가지게 됩니다. 이로 인해 데이터의 크기가 줄어들어 저장 공간을 절약할 수 있습니다. 원본 데이터를 복원하기 위해서는 일부 정보의 손실이 있을 수 있지만, 주성분들이 데이터의 변동성을 잘 설명하므로 중요한 패턴을 유지하면서도 데이터의 차원을 줄일 수 있습니다.

    - 노이즈 제거:

        - PCA는 데이터의 주성분에 해당하는 정보를 추출하고, 주성분이 아닌 성분에 해당하는 잡음이나 무의미한 변동성을 제거합니다. 주성분은 데이터의 변동성이 큰 요소로서 중요한 정보를 포함하고 있기 때문에, 노이즈 성분은 주성분과 비교해 상대적으로 작은 공간을 차지하게 됩니다. 이를 통해 PCA는 데이터의 잡음이나 이상치를 제거하고 신호에 해당하는 부분을 보존하는 효과를 가집니다.

    - 따라서 PCA는 차원 축소를 통해 데이터 압축과 노이즈 제거를 동시에 수행할 수 있습니다. 그러나 압축된 데이터의 복원은 원본 데이터의 일부 정보 손실을 동반하므로, 압축된 데이터로부터 원본 데이터를 완전히 복원하는 것은 불가능합니다. 따라서 압축된 데이터를 활용하는 경우, 주어진 문제나 목적에 맞는 충분한 정보를 보존할 수 있는지 신중하게 고려해야 합니다.


- LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?

    - LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), SVD(Singular Value Decomposition)는 자연어 처리와 토픽 모델링 분야에서 자주 사용되는 약자들입니다. 각각의 뜻과 서로간의 관계에 대해 설명해드리겠습니다:

    - LSA (Latent Semantic Analysis):

        - LSA는 텍스트 문서의 잠재적인 의미를 추출하기 위해 사용되는 통계적인 방법입니다. 주로 텍스트 데이터의 차원 축소에 활용되며, 단어-문서 행렬을 생성한 후, 특잇값 분해(SVD)를 통해 행렬을 저차원의 밀집 표현으로 변환합니다. 이를 통해 문서 간의 의미적 유사성을 계산하거나 검색, 분류 등의 자연어 처리 작업에 활용할 수 있습니다.

    - LDA (Latent Dirichlet Allocation):

        - LDA는 주어진 문서 집합에 대한 토픽 모델링을 수행하는 확률적인 방법론입니다. 토픽 모델링은 문서에 내재된 토픽(주제)을 추론하는 작업으로, LDA는 각 문서에 어떤 토픽이 혼합되어 있는지, 그리고 각 토픽이 어떤 단어들과 연관되어 있는지를 추정합니다. LDA는 단어-문서 행렬을 기반으로 하며, 토픽의 개수, 단어의 확률 분포, 문서의 토픽 분포 등을 추론합니다.

    - SVD (Singular Value Decomposition):

        - SVD는 행렬을 분해하여 행렬의 특성을 추출하는 방법입니다. 주어진 행렬을 세 개의 행렬의 곱으로 분해하는데, 이때 중요한 성질은 특잇값 분해(SVD)라는 과정을 통해 행렬을 주요한 정보를 담고 있는 저차원의 부분공간으로 변환할 수 있다는 것입니다. SVD는 LSA와 밀접한 관련이 있으며, LSA에서 단어-문서 행렬을 SVD로 분해하여 차원 축소를 수행하는 데에 활용됩니다.

    - 요약하자면, LSA는 텍스트 데이터의 의미적 관계를 추출하기 위해 행렬 분해 방법(SVD)을 사용하고, LDA는 주어진 문서 집합에서 토픽을 추론하기 위한 확률적 모델링 기법입니다. LSA와 LDA는 각각 다른 목적과 방식을 가지고 있지만, 텍스트 데이터의 특성을 이해하고 의미적 관계를 추출하는 데에 유용하게 사용됩니다.


- Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?

    - Markov Chain은 상태 간의 전이 확률에 기반하여 다음 상태를 예측하는 확률적인 모델입니다. 이를 고등학생에게 설명하기 위해서는 다음과 같은 방식을 고려할 수 있습니다:

        - 예시와 함께 설명하기: Markov Chain의 개념을 예시를 통해 설명하면 이해가 더욱 쉬울 수 있습니다. 예를 들어, 주사위를 던져서 나오는 숫자가 상태라고 생각해봅시다. 만약 현재 상태가 3이라면, 다음에 어떤 숫자가 나올지 예측할 수 있을까요? 여기서 Markov Chain을 사용하면, 이전 상태(3)에 따라 다음 상태의 확률을 계산하여 예측할 수 있습니다.

        - 상태 전이 다이어그램 활용하기: Markov Chain을 시각적으로 이해할 수 있는 상태 전이 다이어그램을 그려서 설명해줄 수도 있습니다. 각 상태를 노드로 표현하고, 상태 간의 전이 확률을 화살표로 나타내는 방식입니다. 이를 통해 고등학생들은 상태 전이의 개념과 확률적인 모델링을 시각적으로 파악할 수 있습니다.

        - 간단한 문제를 풀어보기: 고등학생들이 Markov Chain을 직접 활용하여 간단한 문제를 풀어보는 것도 도움이 될 수 있습니다. 예를 들어, 어떤 게임의 상황을 Markov Chain으로 모델링하고, 다음 턴에서 어떤 행동을 해야할지 예측하는 문제를 주어 볼 수 있습니다. 이를 통해 실제 응용 가능성을 보여주고, 확률적인 예측의 필요성을 이해할 수 있게 됩니다.

        - 일상 생활 예시로 설명하기: Markov Chain은 우리 일상 생활에서도 적용될 수 있는 개념입니다. 예를 들어, 오늘의 날씨가 비가 오는 경우와 맑은 경우에 따라 내일의 날씨가 어떻게 될지 예측하는 것은 Markov Chain의 아이디어에 근거한 예시입니다. 이를 통해 Markov Chain이 실제로 우리 주변에서 활용되는 개념임을 이해할 수 있습니다.

    - 이러한 방식으로 예시와 시각적인 도구를 활용하며, 일상 생활과 관련시켜 설명하는 것이 고등학생들에게 Markov Chain의 개념을 이해시키는데 도움이 될 것입니다.


- 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?

    - 텍스트 더미에서 주제를 추출하는 것은 텍스트 마이닝의 일부분인 토픽 모델링이라고 불리는 작업입니다. 토픽 모델링은 텍스트 데이터에서 주제를 자동으로 식별하고 그룹화하는 기법입니다. 아래는 토픽 모델링에 접근하는 일반적인 방식입니다:

        - 데이터 전처리: 텍스트 데이터를 전처리하여 불필요한 요소를 제거하고 텍스트를 깨끗하게 정제해야 합니다. 이 과정에는 토큰화, 불용어 제거, 정규화 등이 포함될 수 있습니다.

        - 문서-단어 행렬 생성: 전처리된 텍스트 데이터를 기반으로 문서-단어 행렬을 생성합니다. 이는 각 문서에서 단어의 등장 빈도를 나타내는 행렬입니다.

        - 토픽 모델링 알고리즘 적용: 생성된 문서-단어 행렬에 토픽 모델링 알고리즘을 적용하여 주제를 추출합니다. 대표적인 토픽 모델링 알고리즘으로는 Latent Dirichlet Allocation (LDA)가 있습니다. 이 알고리즘은 주어진 텍스트 데이터에 대해 토픽의 분포와 단어의 분포를 추론하여 주제를 식별합니다.

        - 토픽 해석 및 평가: 추출된 토픽들을 해석하고 각 토픽의 의미와 관련된 단어들을 살펴봅니다. 주제의 의미를 파악하고 해당 토픽들을 평가하여 최종 주제를 결정할 수 있습니다.

        - 결과 시각화: 토픽 모델링 결과를 시각화하여 토픽 간의 관계를 파악하고 주제를 더욱 명확하게 이해할 수 있도록 합니다. 시각화 도구로는 단어 구름(word cloud), 토픽 간의 연관 네트워크 등을 활용할 수 있습니다.

    - 위의 방식을 차례대로 진행하면서 주제를 추출해 나갈 수 있습니다. 토픽 모델링은 텍스트 데이터에서 의미 있는 정보를 추출하는 강력한 도구로 활용될 수 있으며, 주제 분석, 문서 요약, 정보 검색 등 다양한 응용 분야에서 활용될 수 있습니다.


- SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

    - SVM(Support Vector Machine)은 차원을 확장시키는 방식으로 동작하는 이유와 그 장점은 다음과 같습니다:

        - 선형 분리 불가능한 문제 해결: SVM은 기본적으로 선형 분리 가능한 문제를 해결하는 데에 사용됩니다. 그러나 원래 데이터가 선형 분리 불가능한 경우, SVM은 커널 트릭(kernel trick)을 통해 데이터를 고차원 공간으로 매핑합니다. 이렇게 차원을 확장시킴으로써 선형 분리 가능한 문제로 변환하여 해결할 수 있습니다.

        - 차원 확장의 이점: 차원을 확장시키는 것은 고차원 공간에서 데이터를 더 잘 분리할 수 있는 장점을 가지기 때문에 SVM의 성능을 향상시킵니다. 데이터를 고차원 공간으로 매핑하면, 선형 분리 가능성이 높아지고 결정 경계를 더욱 정확하게 찾을 수 있게 됩니다.

        - 마진 최대화: SVM은 클래스 간의 간격(마진)을 최대화하는 결정 경계를 찾는 것을 목표로 합니다. 따라서 SVM은 일반화 성능이 우수하며, 새로운 데이터에 대한 예측 정확도가 높습니다. 마진 최대화는 오버피팅(overfitting)을 방지하고 모델의 일반화 능력을 향상시키는데 도움을 줍니다.

        - 커널 기법의 유연성: SVM은 다양한 커널 함수를 사용할 수 있는 유연성을 가지고 있습니다. 커널 함수는 데이터를 고차원 공간으로 매핑하는 함수로, 비선형 문제를 해결할 수 있게 합니다. 커널 함수를 선택함으로써 SVM은 다양한 데이터 패턴에 적용할 수 있고, 데이터의 복잡한 구조를 학습할 수 있습니다.

    - 따라서 SVM은 선형 분리 가능한 문제뿐만 아니라 비선형 문제에도 적용할 수 있는 강력한 분류 모델입니다. 차원 확장과 커널 트릭을 통해 데이터를 고차원 공간에서 잘 분리할 수 있으며, 일반화 능력이 뛰어나고 복잡한 데이터 구조를 학습할 수 있는 장점을 가지고 있습니다.


- 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.

    - 나이브 베이즈 분류기는 다른 머신 러닝 알고리즘과 비교하여 다음과 같은 장점을 가지고 있습니다:

        - 간단하고 빠른 학습: 나이브 베이즈는 간단하고 직관적인 모델로써, 특성들 간의 독립성 가정을 기반으로 하여 학습합니다. 이로 인해 학습이 빠르고 효율적입니다. 특성 간의 상호작용이나 복잡한 조건부 의존성을 모델링하지 않아도 되기 때문에, 학습 데이터의 크기에 상대적으로 덜 민감하며 작은 데이터셋에서도 잘 동작합니다.

        - 작은 차원에서 좋은 성능: 나이브 베이즈 분류기는 특성 간의 독립성 가정을 사용하기 때문에, 특성이 많은 고차원 데이터보다는 상대적으로 작은 차원의 데이터에서 더 잘 작동합니다. 특히 텍스트 분류와 같은 자연어 처리 문제에서 효과적이며, 많은 수의 단어나 특성이 있는 경우에도 상대적으로 낮은 계산 비용으로 분류를 수행할 수 있습니다.

        - 예측이 빠르고 효율적: 나이브 베이즈 분류기는 각 특성의 조건부 확률을 계산하여 예측을 수행합니다. 이는 예측이 빠르고 메모리 사용량이 적은 것을 의미합니다. 따라서 실시간 예측이나 대규모 데이터셋에서도 효율적으로 적용할 수 있습니다.

        - 작은 데이터셋에서도 효과적: 나이브 베이즈 분류기는 데이터가 적을 때에도 상대적으로 좋은 성능을 발휘합니다. 작은 데이터셋에서도 강건하게 작동하며, 오버피팅의 문제를 완화시킵니다.

        - 이해하기 쉬운 결과 해석: 나이브 베이즈 분류기는 결과를 확률 형태로 제공하여 해석이 용이합니다. 클래스 별로 조건부 확률을 계산하므로, 특정 클래스에 속할 확률을 직관적으로 이해할 수 있습니다.

    - 따라서 나이브 베이즈 분류기는 학습과 예측 속도가 빠르고, 작은 차원에서 강력한 성능을 보이며, 작은 데이터셋에서도 효과적으로 작동하는 등의 장점을 가지고 있습니다.


- 회귀 / 분류시 알맞은 metric은 무엇일까?

    - 회귀 문제에서는 일반적으로 다음과 같은 metric을 사용합니다:

        - 평균 제곱근 오차 (RMSE): 예측값과 실제값 간의 차이를 제곱하여 평균한 뒤, 제곱근을 취한 값입니다. 오차의 크기를 파악할 수 있으며, 큰 오차에 민감합니다. 이상치(outlier)에 민감한 특징이 있습니다.

        - 평균 절대 오차 (MAE): 예측값과 실제값 간의 차이의 절대값을 평균한 값입니다. 오차의 크기를 파악할 수 있으며, RMSE보다 이상치에 덜 민감합니다.

        - R 제곱 (R-squared): 예측값이 실제값의 변동을 얼마나 설명하는지를 나타내는 지표입니다. 0과 1 사이의 값을 가지며, 1에 가까울수록 모델이 더 좋은 예측을 수행하는 것으로 판단됩니다. 하지만 이 지표는 설명력이 강한 모델에만 적합하고, 데이터의 특성에 따라 해석이 달라질 수 있습니다.

    - 분류 문제에서는 다음과 같은 metric을 사용합니다:

        - 정확도 (Accuracy): 전체 샘플 중 올바르게 예측한 샘플의 비율입니다. 데이터 클래스의 균형이 잘 맞을 때 유용한 지표입니다. 그러나 클래스 불균형 문제가 있을 경우, 정확도만으로 모델의 성능을 평가하는 것은 적절하지 않을 수 있습니다.

        - 정밀도 (Precision): 양성으로 예측한 샘플 중 실제로 양성인 샘플의 비율입니다. 거짓 양성을 줄이는 데에 초점을 맞추는 경우 유용한 지표입니다.

        - 재현율 (Recall): 실제 양성인 샘플 중 양성으로 예측한 샘플의 비율입니다. 거짓 음성을 줄이는 데에 초점을 맞추는 경우 유용한 지표입니다.

        - F1 점수 (F1 Score): 정밀도와 재현율의 조화 평균으로 계산되는 지표입니다. 정밀도와 재현율을 모두 고려하여 평가하고자 할 때 사용됩니다.

    - 다만, 실제 문제에 따라서는 특수한 metric이 사용될 수 있으며, 이는 해당 문제의 목적과 요구 사항에 따라 결정되어야 합니다.


- Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.

    - Association Rule은 데이터 마이닝에서 사용되는 규칙 기반 분석 방법 중 하나입니다. Association Rule의 세 가지 중요한 개념인 Support, Confidence, Lift에 대해 설명드리겠습니다:

        - Support (지지도):
        
            - Support는 주어진 데이터 집합에서 특정 아이템 집합이 발생하는 빈도를 측정하는 지표입니다. Support는 아이템 집합이 전체 데이터에서 차지하는 비율로 표현되며, 보통 백분율로 표기됩니다. Support(A)는 아이템 집합 A가 발생하는 비율을 의미합니다. Support는 어떤 아이템이 다른 아이템과 연관되어 발생하는 정도를 나타내는데 사용됩니다.

        - Confidence (신뢰도):

            - Confidence는 주어진 조건 아이템 집합이 발생했을 때 결과 아이템 집합이 발생하는 조건부 확률을 측정하는 지표입니다. Confidence는 아이템 집합 A가 발생했을 때 아이템 집합 B가 발생할 확률을 나타냅니다. Confidence(A → B)는 아이템 집합 A가 발생했을 때 아이템 집합 B가 발생하는 비율을 의미합니다. Confidence는 연관 규칙의 강도를 나타내며, 높은 Confidence 값은 아이템 간의 강한 관련성을 나타냅니다.

        - Lift (향상도):

            - Lift는 아이템 집합 A와 아이템 집합 B 사이의 연관성을 측정하는 지표입니다. Lift는 Confidence를 Support로 나눈 값으로 계산됩니다. Lift(A → B)는 아이템 집합 A가 주어졌을 때 아이템 집합 B의 발생이 기본적인 발생 확률보다 얼마나 더 높은지를 나타냅니다. Lift 값이 1보다 크면 A와 B가 양의 상관 관계를 가지며, Lift 값이 1보다 작으면 A와 B가 음의 상관 관계를 가집니다. Lift 값이 1에 가까울수록 A와 B는 서로 독립에 가까워집니다.

    - Support, Confidence, Lift는 Association Rule의 강도와 신뢰도를 측정하는데 사용되는 지표들로, 연관 규칙 분석을 통해 데이터 세트에서 유용한 규칙을 찾아내는 데에 활용됩니다.


- 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?

    - Newton's Method와 Gradient Descent는 둘 다 최적화 기법 중 일부입니다. 각각의 방법에 대해 설명해드리겠습니다:

        - Newton's Method:

            - Newton's Method는 함수의 극소점 또는 극대점을 찾기 위해 사용되는 반복적인 최적화 알고리즘입니다. 이 방법은 함수의 미분 값과 2차 도함수를 사용하여 최적화 과정을 진행합니다. Newton's Method는 현재 위치에서의 접선을 사용하여 다음 위치를 결정하고, 점진적으로 극소점 또는 극대점에 도달합니다.

            - Newton's Method는 수렴 속도가 빠르고, 2차 도함수 정보를 활용하여 더 정확한 극소점 또는 극대점을 찾을 수 있습니다. 하지만 함수의 2차 도함수가 계산 가능하고 연속인 경우에만 사용할 수 있으며, 초기 추정치에 따라 다른 극소점 또는 극대점에 수렴할 수 있는 문제가 있을 수 있습니다.

        - Gradient Descent:
        
            - Gradient Descent는 주어진 함수의 극소점을 찾기 위한 최적화 알고리즘입니다. 이 방법은 함수의 기울기(gradient)를 사용하여 최적화 과정을 진행합니다. Gradient Descent는 현재 위치에서 기울기의 반대 방향으로 이동하여 극소점을 찾는 방식으로 작동합니다.

            - Gradient Descent는 단순하고 일반적으로 많이 사용되는 최적화 기법입니다. 하지만 수렴 속도가 느리고, 극소점이나 극대점으로 수렴하지 않고 지역 최적해에 수렴할 수 있는 문제가 있을 수 있습니다. Gradient Descent의 성능은 학습률(learning rate)에 따라 크게 영향을 받으며, 적절한 학습률을 선택하는 것이 중요합니다.

    - 두 방법은 각각의 장단점을 가지고 있으며, 최적화하려는 함수의 특성과 제약사항에 따라 적합한 방법을 선택해야 합니다. Newton's Method는 보다 정확한 결과를 원하거나 2차 도함수 정보를 활용할 수 있는 경우에 유용하며, Gradient Descent는 단순하고 일반적인 경우에 적용 가능한 방법입니다.


- 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

    - 머신 러닝과 통계는 데이터 분석과 모델링에 대한 두 가지 다른 접근 방식을 나타냅니다. 각각의 접근 방식은 목적, 방법론, 주요 관점 등에서 차이를 가지고 있습니다.

    - 목적:

        - 머신 러닝: 머신 러닝은 데이터로부터 패턴을 학습하여 예측, 분류, 군집 등의 작업을 수행하는 것이 주요 목적입니다. 데이터에 내재된 구조나 특성을 탐색하고, 복잡한 모델을 구축하여 예측 능력을 최적화합니다.

        - 통계: 통계는 데이터로부터 모집단에 대한 추론을 수행하는 것이 주요 목적입니다. 표본에서 모집단의 특성을 추정하고, 추정된 결과에 대한 불확실성을 평가하며, 가설 검정 등을 통해 통계적 결론을 도출합니다.

    - 방법론:

        - 머신 러닝: 머신 러닝은 주로 기계 학습 알고리즘을 사용하여 데이터에서 패턴을 학습합니다. 지도 학습, 비지도 학습, 강화 학습 등 다양한 방법론과 알고리즘이 사용됩니다. 데이터의 특성을 탐지하고 예측 모델을 구축하는 과정에 중점을 둡니다.

        - 통계: 통계는 주로 확률 모형과 통계적 추론 기법을 사용합니다. 표본 추출, 가설 설정, 추정, 가설 검정, 신뢰 구간 등의 통계적 기법을 활용하여 데이터를 분석하고 모델링합니다.

    - 주요 관점:

        - 머신 러닝: 머신 러닝은 데이터에 포함된 패턴과 관계를 중요시합니다. 모델의 예측 성능과 일반화 능력에 초점을 두며, 모델의 복잡성과 일반화 오차 간의 균형을 고려합니다.

        - 통계: 통계는 데이터의 불확실성과 통계적 결론의 신뢰도를 중요시합니다. 데이터의 변동성, 표본 크기, 통계적 유의성 등을 고려하여 모델의 신뢰도와 결과의 해석 가능성을 평가합니다.

    - 머신 러닝과 통계는 상호 보완적인 관점을 가지고 있으며, 데이터 분석의 목적과 상황에 따라 어떤 접근 방식을 선택할지 결정할 수 있습니다. 실제로 많은 경우 머신 러닝과 통계를 조합하여 데이터 분석과 모델링을 수행하는 경우도 많이 있습니다.


- 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?

    - 전통적인 인공신경망(Deep Learning 이전의 네트워크)은 몇 가지 일반적인 문제점을 가지고 있습니다. 여기에는 다음과 같은 요소들이 포함됩니다:

        - 과적합(Overfitting): 인공신경망은 매우 복잡한 모델이기 때문에, 훈련 데이터에 지나치게 적합되는 경향이 있습니다. 이로 인해 새로운 데이터에 대한 일반화 성능이 떨어질 수 있습니다.

        - 학습 속도와 수렴 문제: 일부 경우에서, 인공신경망은 학습 속도가 느리고 수렴하기까지 많은 시간이 걸릴 수 있습니다. 특히, 깊은 네트워크의 경우 초기 가중치 설정과 학습률 조정이 중요합니다.

        - 데이터 부족 문제: 인공신경망은 많은 양의 훈련 데이터를 필요로 할 수 있습니다. 작은 규모의 데이터셋에서는 과적합이 발생할 수 있으며, 신경망이 적절한 일반화를 달성하기 어려울 수 있습니다.

        - 설명 가능성 부족: 인공신경망은 매우 복잡한 모델이기 때문에, 그 내부 작동 방식을 이해하고 해석하기 어려울 수 있습니다. 이로 인해 모델의 결정 과정을 설명하거나 예측 결과를 해석하는 것이 어려울 수 있습니다.

        - 하이퍼파라미터 튜닝: 인공신경망은 다양한 하이퍼파라미터(예: 레이어 수, 뉴런 수, 학습률 등)를 조정해야 할 수 있습니다. 이러한 하이퍼파라미터를 최적화하는 것은 어려울 수 있으며, 신경망의 성능에 큰 영향을 미칩니다.

        - 계산 자원 요구: 대규모 인공신경망의 경우, 학습과 추론을 위해 많은 계산 자원과 컴퓨팅 파워가 필요할 수 있습니다. 이는 하드웨어나 인프라 구성에 추가적인 비용과 제약 사항을 요구할 수 있습니다.

    - 이러한 문제점들은 딥러닝의 발전과 함께 다양한 기법과 알고리즘의 개발을 통해 해결되고 완화되고 있습니다. 하지만 일부 문제는 여전히 도전적인 과제로 남아 있습니다.


- 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?

    - 저는 현재 Deep Learning 계열의 혁신의 근간은 크게 두 가지로 생각합니다:

        - 대규모 데이터셋과 컴퓨팅 파워: Deep Learning은 많은 양의 데이터를 필요로 합니다. 현대의 딥러닝 모델은 대용량 데이터셋을 이용하여 학습됩니다. 빅데이터의 확장과 클라우드 컴퓨팅의 발전은 대규모 데이터셋과 고성능 컴퓨팅 자원을 활용할 수 있게 되었습니다. 이를 통해 딥러닝 모델의 규모를 확장하고 복잡한 문제를 다룰 수 있게 되었습니다.

        - 신경망 구조와 알고리즘의 발전: 딥러닝에서 사용되는 신경망 구조와 알고리즘은 지속적으로 발전하고 개선되고 있습니다. Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Generative Adversarial Networks (GAN) 등의 신경망 구조가 개발되었고, 이를 기반으로 한 다양한 알고리즘이 제안되고 발전되고 있습니다. 또한, 초기화 방법, 활성화 함수, 정규화 기법, 옵티마이저 등의 핵심 요소들도 계속해서 연구되고 개선되어 성능과 효율성을 향상시켰습니다.

    - 이러한 기반 위에 다양한 혁신과 발전이 이루어지고 있습니다. 예를 들면, Transfer Learning, Self-Supervised Learning, Attention Mechanism, Transformer Architecture, GPT (Generative Pre-trained Transformer) 등은 현재 Deep Learning 분야에서 주목받고 있는 주요 혁신 중 일부입니다. 이러한 혁신들은 더 나은 성능, 더 효율적인 학습, 더 유연한 모델 구조 등을 가능하게 하며, 다양한 응용 분야에서의 실질적인 성과를 이끌어내고 있습니다.


- ROC 커브에 대해 설명해주실 수 있으신가요?

    -


- 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?

    -


- K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

    -


- L1, L2 정규화에 대해 설명해주세요.

    -


- Cross Validation은 무엇이고 어떻게 해야하나요?

    -


- XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

    -


- 앙상블 방법엔 어떤 것들이 있나요?

    -


- feature vector란 무엇일까요?

    -


- 좋은 모델의 정의는 무엇일까요?

    -


- 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?

    -


- 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?

    -


- OLS(ordinary least squre) regression의 공식은 무엇인가요?

    -



🧠 Deep Learning

- 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?

    -


- Cost Function과 Activation Function은 무엇인가요?

    -


- Tensorflow, PyTorch 특징과 차이가 뭘까요?

    -


- Data Normalization은 무엇이고 왜 필요한가요?

    -


- 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)

    -


- 오버피팅일 경우 어떻게 대처해야 할까요?

    -


- 하이퍼 파라미터는 무엇인가요?

    -


- Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?

    -


- 볼츠만 머신은 무엇인가요?

    -


- TF, PyTorch 등을 사용할 때 디버깅 노하우는?

    -


- 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?

    -


- 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?

    -


- Non-Linearity라는 말의 의미와 그 필요성은?

    -


- ReLU로 어떻게 곡선 함수를 근사하나?

    -


- ReLU의 문제점은?

    -


- Bias는 왜 있는걸까?

    -


- Gradient Descent에 대해서 쉽게 설명한다면?

    -


- 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?

    -


- GD 중에 때때로 Loss가 증가하는 이유는?

    -


- Back Propagation에 대해서 쉽게 설명 한다면?

    -


- Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?

    -


- GD가 Local Minima 문제를 피하는 방법은?

    -


- 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?

    -


- Training 세트와 Test 세트를 분리하는 이유는?

    -


- Validation 세트가 따로 있는 이유는?

    -


- Test 세트가 오염되었다는 말의 뜻은?

    -


- Regularization이란 무엇인가?

    -


- Batch Normalization의 효과는?

    -


- Dropout의 효과는?

    -


- BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?

    -


- GAN에서 Generator 쪽에도 BN을 적용해도 될까?

    -


- SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?

    -


- SGD에서 Stochastic의 의미는?

    -


- 미니배치를 작게 할때의 장단점은?

    -


- 모멘텀의 수식을 적어 본다면?

    -


- 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?

    -


- 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?

    -


- Back Propagation은 몇줄인가?

    -


- CNN으로 바꾼다면 얼마나 추가될까?

    -


- 간단한 MNIST 분류기를 TF, PyTorch 등으로 작성하는데 몇시간이 필요한가?

    -


- CNN이 아닌 MLP로 해도 잘 될까?

    -


- 마지막 레이어 부분에 대해서 설명 한다면?

    -


- 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?

    -


- 딥러닝할 때 GPU를 쓰면 좋은 이유는?

    -


- GPU를 두개 다 쓰고 싶다. 방법은?

    -


- 학습시 필요한 GPU 메모리는 어떻게 계산하는가?

    -
