{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분리기\n",
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 검사기\n",
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "\n",
    "# 오류 발생\n",
    "# 구글링 결과 \n",
    "# !pip uninstall imgaug\n",
    "# !pip install imgaug==0.2.5\n",
    "# 위 코드로 해결된다고 해서 시도했으나 안 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춤법 검사기\n",
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외래어 사전\n",
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분리기\n",
    "!git clone https://github.com/kakao/khaiii.git\n",
    "!pip install cmake\n",
    "!mkdir build\n",
    "!cd build && cmake /content/khaiii\n",
    "!cd /content/build/ && make all\n",
    "!cd /content/build/ && make resource\n",
    "!cd /content/build && make install\n",
    "!cd /content/build && make package_python\n",
    "!pip install /content/build/package_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./기아+기아.csv', encoding='utf-8', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "# 문장단위 분리\n",
    "sentence_tokenized_text = []\n",
    "for comment in df.comments:\n",
    "    comment = comment.strip()\n",
    "    for sent in kss.split_sentences(comment):\n",
    "        sentence_tokenized_text.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 설정\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 대체재 설정\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \n",
    "\"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", \n",
    "'“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', \n",
    "'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', \n",
    "'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 정리\n",
    "def clean_punc(text, punct, mapping):\n",
    "    \n",
    "\t\t# 특수문자를 특수문자 대체제로 변환\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    # 특수문자 분리\n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 실행\n",
    "cleaned_corpus = []\n",
    "for sent in sentence_tokenized_text:\n",
    "    cleaned_corpus.append(clean_punc(sent, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "for i in range(0, 10):\n",
    "    print(cleaned_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 정규표현식 문자열 처리\n",
    "def clean_text(texts):\n",
    "    corpus = []\n",
    "    for i in range(0, len(texts)):\n",
    "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) # 특수문자 제거\n",
    "        review = re.sub(r'\\d+','', str(texts[i])) # 숫자제거\n",
    "        review = review.lower() # 소문자 변환\n",
    "        review = re.sub(r'<[^>]+>','',review) # html 태그 제거\n",
    "        review = re.sub(r'\\s+', ' ', review) # 공백 제거\n",
    "        review = re.sub(r\"^\\s+\", '', review) # 문자열 앞의 공백 제거\n",
    "        review = re.sub(r'\\s+$', '', review) # 문자열 뒤의 공백 제거\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "basic_preprocessed_corpus = clean_text(cleaned_corpus)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(basic_preprocessed_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import spacing\n",
    "from hanspell import spell_checker\n",
    "from soynlp.normalizer import *\n",
    "\n",
    "# 외래어 사전 불러오기\n",
    "lownword_map = {}\n",
    "lownword_data = open('/content/confused_loanwords.txt', 'r', encoding='utf-8')\n",
    "lines = lownword_data.readlines()\n",
    "\n",
    "# 외래어 사전 재구성\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    miss_spell = line.split('\\t')[0] # 잘못된 외래어 분리\n",
    "    ori_word = line.split('\\t')[1] # 수정 후 외래어 분리\n",
    "\t\t# 맞춤법이 틀린 외래어를 key로 설정, 수정 후 외래어를 value로 설정해 딕셔너리 생성\n",
    "    lownword_map[miss_spell] = ori_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        spaced_text = spacing(sent) # 띄어쓰기\n",
    "        spelled_sent = spell_checker.check(sent) # 맞춤법 검사\n",
    "        checked_sent = spelled_sent.checked \n",
    "\t\t\t\t# 정규화 ex) 와하하하하하하핫 = 와하하핫\n",
    "        normalized_sent = repeat_normalize(checked_sent) \n",
    "        for lownword in lownword_map:\n",
    "\t\t\t\t\t\t# 외래어 사전 딕셔너리 key에 해당하는 단어가 있을 경우 그에 맞는 value 단어로 변환\n",
    "            normalized_sent = normalized_sent.replace(lownword, lownword_map[lownword])\n",
    "        corpus.append(normalized_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 실행\n",
    "spell_preprocessed_corpus = spell_check_text(basic_preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "api = KhaiiiApi()\n",
    "\n",
    "# 품사 설정\n",
    "# 일반 명사, 고유 명사, 의존 명사, 동사, 형용사, 보조 용언, 일반 부사, 접속 부사, 동사 파생 접미사, 형용사 파생 접미사\n",
    "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "\n",
    "def pos_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        pos_tagged = ''\n",
    "        for word in api.analyze(sent): # 띄어쓰기 기준 분리, 형태소 + 품사 형태 반환\n",
    "            for morph in word.morphs:\n",
    "                if morph.tag in significant_tags:\n",
    "\t\t\t\t\t\t\t\t\t\t# 형태소 + / + 품사 + ' '\n",
    "                    pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
    "        corpus.append(pos_tagged.strip())\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용\n",
    "pos_tagged_corpus = pos_text(spell_preprocessed_corpus)\n",
    "\n",
    "for i in range(0, 30):\n",
    "    print(pos_tagged_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동사 원형 복원 기준\n",
    "#NNG(일반 명사)|NNP(고유 명사)|NNB(의존 명사) + XSV(동사 파생 접미사)|XSA(형용사 파생 접미사) --> NNG(일반 명사)|NNP(고유 명사)|NNB(의존 명사) + XSV(동사 파생 접미사)|XSA(형용사 파생 접미사) + 다\n",
    "#NNG(일반 명사)|NNP(고유 명사)|NNB(의존 명사) + XSA(형용사 파생 접미사) + VX(보조 용언) --> NNG(일반 명사)|NNP(고유 명사) + XSA(형용사 파생 접미사) + 다\n",
    "#VV(동사) --> VV(동사) + 다\n",
    "#VX(보조 용언) --> VX(보조 용언) + 다\n",
    "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.') # 명사 + 접미사\n",
    "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX') # 명사 + 형용사 파생 접미사 + 보조 용언\n",
    "p3 = re.compile('[가-힣A-Za-z0-9]+/VV') # 동사\n",
    "p4 = re.compile('[가-힣A-Za-z0-9]+/VX') # 보조 용언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        ori_sent = sent\n",
    "        mached_terms = re.findall(p1, ori_sent) # 위에 컴파일한 것과 일치하는 경우 찾기\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '): # 공백 기준 분리\n",
    "                lemma = term.split('/')[0] # 단어\n",
    "                tag = term.split('/')[-1] # 품사\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        mached_terms = re.findall(p2, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p3, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p4, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        corpus.append(ori_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 실행\n",
    "stemming_corpus = stemming_text(pos_tagged_corpus)\n",
    "\n",
    "for i in range(0, 30):\n",
    "    print(stemming_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 설정, 한국어 코퍼스 최빈도어\n",
    "stopwords = ['이/VCP','나오/VV','있/VA','가지/VV','하/VV','씨/NNB','것/NNB','시키/XSV','들/XSN',\n",
    "'만들/VV','그/MM','지금/NNG'\t,'되/VV','생각하/VV','수/NNB','그러/VV'\t,'이/NP','속/NNG','보/VX',\n",
    "'하나/NR','않/VX','집/NNG','없/VA','살/VV','나/NP','모르/VV','사람/NNG','적/XSN','주/VV','월/NNB',\n",
    "'아니/VCN','데/NNB','등/NNB','자신/NNG','같/VA','안/MAG','우리/NP','어떤/MM','때/NNG','내/NP','년/NNB',\n",
    "'내/VV','가/VV','경우/NNG','한/MM','명/NNB','지/VX','생각/NNG','대하/VV','시간/NNG','오/VV','그녀/NP',\n",
    "'말/NNG','다시/MAG','일/NNG','이런/MM','그렇/VA','앞/NNG','위하/VV','보이/VV','때문/NNB','번/NNB',\n",
    "'그것/NP','나/VX','두/VV','다른/MM','말하/VV','어떻/VA','알/VV','여자/NNG','그러나/MAJ','개/NNB',\n",
    "'받/VV','전/NNG','못하/VX','들/VV','일/NNB','사실/NNG','그런/MM','이렇/VA','또/MAG','점/NNG','문제/NNG',\n",
    "'싶/VX','더/MAG','말/VX','사회/NNG','정도/NNG','많/VA','좀/MAG','그리고/MAJ','원/NNB','좋/VA','잘/MAG',\n",
    "'크/VA','통하/VV','따르/VV','소리/NNG','중/NNB','놓/VX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        modi_sent = []\n",
    "        for word in sent.split(' '): # 공백 기준 분리\n",
    "            if word not in stopwords:\n",
    "                modi_sent.append(word) # 불용어가 아닌 단어들을 modi_sent에 추가\n",
    "        corpus.append(' '.join(modi_sent)) # list에 공백을 구분자로 문자열로 변환\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 실행\n",
    "removed_stopword_corpus = remove_stopword_text(stemming_corpus)\n",
    "\n",
    "for i in range(0, 30):\n",
    "    print(removed_stopword_corpus[i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
