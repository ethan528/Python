{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hanspell import spell_checker\n",
    "from konlpy.tag import Okt, Hannanum\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from krwordrank.sentence import summarize_with_sentences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from IPython.display import Image\n",
    "from mlxtend.plotting import scatterplotmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonata = pd.read_csv('/Users/parkjubro/Desktop/파이널/합본1/현대+쏘나타_label.csv') # 데이터 불러오기\n",
    "sonata = sonata.drop('Unnamed: 0', axis = 'columns')\n",
    "sonata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "#이모티콘 제거 (아이폰 이모티콘들은 따로 코드가 존재)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00010000-\\U0010FFFF\"                   \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#분석에 어긋나는 불용어구 제외 (특수문자, 의성어)\n",
    "han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,\".\\n\\r#\\ufeff\\u200d\\xa0]')\n",
    "\n",
    "# train data(sonata)에 있는 댓글들 전처리를 위해 리스트화\n",
    "comment_list = [] \n",
    "for i in range(len(sonata)):\n",
    "    comment_list.append(sonata['comments'].iloc[i])\n",
    "    \n",
    "# 전처리 1 (댓글 이모지 밑 기호 정리 후 정규화)\n",
    "comment_result = [] \n",
    "\n",
    "for i in comment_list:\n",
    "    tokens = re.sub(emoji_pattern,\"\",i) # 이모지 패턴 적용\n",
    "    tokens = re.sub(han,\"\",tokens) # han 적용\n",
    "    tokens = re.sub('[-=+,#/\\?:^.@*\\\"※%~∼ㆍ!【】』㈜©囹圄秋 ■◆◇▷▶◁◀ △▲▽▼<>‘|\\(\\)\\[\\]`\\'…》→←↑↓↔〓♤♠♡♥♧♣⊙◈▣◐◑☆★\\”\\“\\’·※~ ! @ # $ % ^ & * \\ \" ]', ' ', tokens)\n",
    "    # 기타 특수문자들 제거\n",
    "    tokens = okt.normalize(tokens) # 정규화\n",
    "    comment_result.append(tokens)\n",
    "\n",
    "comment_result\n",
    "\n",
    "# 전처리 2 (spell_checker 활용하여 오탈자 수정)\n",
    "checked_list = [] \n",
    "for comment in tqdm(comment_result):\n",
    "    sent = comment\n",
    "    try:\n",
    "        spelled_sent = spell_checker.check(sent)\n",
    "        checked_sent = spelled_sent.checked\n",
    "        checked_list.append(checked_sent)\n",
    "    except:\n",
    "        print(sent)\n",
    "        checked_list.append(sent)\n",
    "\n",
    "# 학습을 위해 댓글과 라벨 컬럼만 있는 새로운 샘플 데이터프레임 생성\n",
    "sample = pd.DataFrame() \n",
    "sample['comments'] = checked_list\n",
    "sample['label'] = sonata['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trian data 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# w2index\n",
    "word_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 최대 길이에 따라 훈련 데이터 크기 설정 \n",
    "long = max(len(sample) for sample in X_train_encoded) # 전처리 과정에서 댓글 최대 길이가 가끔 달라지기 때문에 long 변수 설정함\n",
    "max_len = long\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
    "\n",
    "# long = 137\n",
    "# 훈련 데이터 크기 : (2429, 137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "dropout_ratio = 0.3\n",
    "num_filters = 32\n",
    "kernel_size = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(dropout_ratio)) # 과적합 방지를 위해 validation loss가 3번 증가할 때 자동으로 학습 중지\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_padded, y_train, epochs=13, batch_size=137, validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/Users/parkjubro/Desktop/파이널/데이터들/test.csv').astype(str)\n",
    "x_test = test_df['comments'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_test)\n",
    "X_test_encoded = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_long = max(len(sample) for sample in X_test_encoded)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = test_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = prediction\n",
    "test_df = test_df.drop(columns = 'Unnamed: 0', axis = 1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts = test_df['comments'].tolist()\n",
    "penalty = lambda x:0 if (10 <= len(x) <= 150) else 1\n",
    "stopwords = {'너무', '리뷰', '진짜', '기자님', '정말', '많이', '영상', '보고', \n",
    "             '이번', '같은', '그냥', '시승기', '보면', '하는', '김한용', '근데', \n",
    "             '아니', '다른', '역시', '항상', '한상기', '라이', '있는', '생각', \n",
    "             '정도', '나오', '지금', '같아', '이제', '있습니다', '요즘', '그리고',\n",
    "            '어떤', '이쁘', '보이', '보니', '때문에', '솔직히', '뭔가', '설명',\n",
    "            '같네요', '하고', '감사', '특히', '봤습니', '정도', '차가', '차를', \n",
    "             '엄청', '이런', '보는', '훨씬', '까요', '아닌', '20', '현대', '벤츠',\n",
    "             '없는', '현기', '한국', '그런', '10', '아반떼', '운전', '하나', '저도',\n",
    "             '저는', '이거', '소리', '이상', '그래', '차는', '있어', '이렇게', '아직',\n",
    "             '들어', '만들', '타는', '합니다', '우리', '차이' ,'일본', '아우디', '볼보', \n",
    "            '갑니다', '어떻게', '다시', '구매', '제가', '가장', '조금', '하면', '미국',\n",
    "             '국내', '한번', '기아', '신형', '그랜저', '쌍용', '30', '거의', '하지', 'the',\n",
    "             '스포', '사는', '많은', '봤습니다', '없어', '바로', '되는', '혹시', '계속',\n",
    "             '확실히', '같네', '텐데', '건가', '그렇', '오늘', '무슨', '국산', '해도', '없고',\n",
    "            '광고', '대한', '아주', '싶네요', '부분', '얼마나', '제일',' 아무리', '궁금', '모르',\n",
    "            '개인', '아무리', '타고', '것도', '나온'}\n",
    "\n",
    "keywords, sents = summarize_with_sentences(\n",
    "    texts,\n",
    "    penalty=penalty,\n",
    "    stopwords = stopwords,\n",
    "    diversity=0.5,\n",
    "    num_keywords=100,\n",
    "    num_keysents=10, # 키워드 분석을 통해 가장 핵심적인 문장들 뽑아내는 개수\n",
    "    verbose=False\n",
    ")\n",
    "for word, r in sorted(keywords.items(), key = lambda x:x[1], reverse=True)[:30]:\n",
    "                     print('%8s:\\t%.4f' % (word, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = []\n",
    "for comment in sample['comments']:\n",
    "    if comment in checked_list:\n",
    "        new_label.append(0)\n",
    "    else:\n",
    "        new_label.append(1)\n",
    "\n",
    "sonata['new_label'] = new_label\n",
    "sonata['new_label'].value_counts()\n",
    "\n",
    "# 1    1556\n",
    "# 0    1481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.read_csv('/Users/parkjubro/Desktop/파이널/데이터들/preprocessed_kb_0518.csv')\n",
    "df2 = car_df.drop_duplicates() # 중복값 제거\n",
    "df3 = df2[['car_name','depreciation','year','use','mileage','car_type','insurance']] # 차량모델, 가격, 연식, 사용연수, 주행거리, 차종, 사고여부만 뽑음 (중요한 영향변수)\n",
    "df3 = df3.reset_index() # 인덱스 초기화\n",
    "df3 = df3.drop(columns = 'index', axis = 1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "train_scaled = std.fit_transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_data = df5.drop(columns = 'depreciation')\n",
    "y_data = df5['depreciation']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2 , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 선형회귀\n",
    "linear = LinearRegression()\n",
    "linear.fit(x_train, y_train)\n",
    "\n",
    "linear.score(x_train, y_train)\n",
    "# 0.9145135460000929\n",
    "\n",
    "\n",
    "\n",
    "cross_val_score(linear, x_train, y_train, cv = 3)\n",
    "\n",
    "# array([0.90714164, 0.90488488, 0.90672457])\n",
    "\n",
    "linear.score(x_test, y_test)\n",
    "\n",
    "# -67864608.4408464 -> 훈련값은 정확하나 예측값은 매우 부정확"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfr = RandomForestRegressor(random_state = 0)\n",
    "rfr.fit(x_train, y_train)\n",
    "rfr.score(x_train, y_train)\n",
    "\n",
    "# 0.9887830386703866\n",
    "\n",
    "cross_val_score(rfr, x_train, y_train, cv = 3)\n",
    "\n",
    "# array([0.91676753, 0.91665102, 0.91727352])\n",
    "\n",
    "rfr.score(x_test, y_test)\n",
    "\n",
    "# 0.9173439359185369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#수치형 변수들만 가지고 모델링\n",
    "#kb에 use, mileage, year, new_price, depreciation, forecast_min, forecast_max, car_cc 포함\n",
    "y=kb[['price']].to_numpy()\n",
    "kb=kb.drop(columns=['price','trans','loss','flood','usage','change','insurance','sales_corp','sales_loca','options','car_area','car_no','car_brand','car_name','name_datailed','fuel','car_type','color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, train data 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.85, random_state=1)\n",
    "\n",
    "lr = LinearRegression(fit_intercept = True, normalize= True, copy_X=True)\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(min_samples_leaf=10, min_samples_split=5,learning_rate=0.5,max_depth=3, n_estimators=1000)\n",
    "gb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다중회귀\n",
    "lm = LinearRegression()\n",
    "X = kb[['mileage']]\n",
    "Y = kb['price']\n",
    "Z = kb[['year', 'use', 'depreciation', 'new_price']]\n",
    "lm.fit(X,Y)\n",
    "lm.fit(Z, kb['price'])\n",
    "Y = lm.predict(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "ax1 = sns.distplot(kb['price'], hist=False, color=\"r\", label=\"Actual Value\")\n",
    "sns.distplot(Y, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n",
    "\n",
    "plt.title('Actual vs Fitted Values for Price')\n",
    "plt.xlabel('Price (in dollars)')\n",
    "plt.ylabel('Proportion of Cars')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = re.sub(r'[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    labeling = str(\"{:.2f}% 확률로 긍정 댓글입니다.\\n\".format(score * 100))\n",
    "  else:\n",
    "    labeling = str(\"{:.2f}% 확률로 부정 댓글입니다.\\n\".format((1 - score) * 100))\n",
    "  return labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = youtube_df['comments']\n",
    "youtube_df['labeling'] = ''\n",
    "\n",
    "for idx, comment in enumerate(words):\n",
    "  if '궁금' in comment: # '?' 추가할까? 고민해보기\n",
    "    youtube_df['labeling'][idx] = -1\n",
    "    print(youtube_df.iloc[idx])\n",
    "  else:\n",
    "    labeling = sentiment_predict(comment)\n",
    "    if '긍정' in labeling:  \n",
    "      youtube_df['labeling'][idx] = 0\n",
    "      print(youtube_df.iloc[idx])\n",
    "    elif '부정' in labeling:\n",
    "      youtube_df['labeling'][idx] = 1\n",
    "      print(youtube_df.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_df['comments'] = youtube_df['comments'].str.replace(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "youtube_df['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanspell_sent_lst = []\n",
    "\n",
    "for i in words[:100]:\n",
    "    \n",
    "  spelled_sent = spell_checker.check(i) # 맞춤법 검사\n",
    "  hanspell_sent = spelled_sent.checked # 띄어쓰기 교정\n",
    "  hanspell_sent_lst.append(hanspell_sent)\n",
    "\n",
    "  print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 설정\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "han = Hannanum()\n",
    "\n",
    "youtube_review_han_nouns = []\n",
    "\n",
    "for i in hanspell_sent_lst:\n",
    "    tokenized_sentence = han.nouns(i)\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    youtube_review_han_nouns.append(stopwords_removed_sentence)\n",
    "\n",
    "    # 글자수 한자리 단어, 'ㅋ'이 포함된 단어은 불용어 사전에 넣기\n",
    "    for j in stopwords_removed_sentence:\n",
    "      if len(j) <= 1 or 'ㅋ' in j:\n",
    "        stopwords.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)\n",
    "\n",
    "# 결과\n",
    "['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '백', '차', '대', '뒤', '나', '것', '편', '걸', '전', '보', '듯', '님', '중', '뭐', '저', '저', '둘', '유', '데', '수', '리', '카', '그', '파', '지', '형', '화', '첨', '거', '두', '후', '킹', '개', '욕', '잠', '확', '쪽', 'ㅋ', '흠', '되', '4', '옆', '돈', '타', '칸', '눈', '내', '줄', '분', '손', '등', '동', '체', '너', 'ㅎ', '낫', '생', '밖', '맛', '드', '앞', '핫', 'ㅋㅋㅋ', 'ㅋㅋㅋㅋㅋ', '궁금하네욬ㅋ', '있을깤ㅋ', '못해욧ㅋㅋㅋㅋ', '로맠ㅋㅋ', '2억ㅋ', '소문잌ㅋ']\n",
    "\n",
    "\n",
    "print(youtube_review_han_nouns)\n",
    "\n",
    "# 결과\n",
    "[['파나메라', '패스트', '스타일'], ['진짜', '최고다ㅠㅠ타보'], [], ['기업', '총수', '대형', '세단', '기사님', '운전'], ['날씨', '고생', '영상', '감사'], ['간결', '설명'], ['1255', '이차', '전동', '트렁크', '특유', '과감', '한상기', '트렁크'], ['ㅎㅎㅎ'], ['잔고장', '정도'], ['최고'], ['전면', '디자인', '터보', '전용', '일반', '파나메라', '스포츠', '범퍼', '옵션', '선택', '궁금'], ['날씨'], ['며칠', '시내', '주행', '결함', '신형', '포르쉐'], ['진짜', '파라네라', '완전', '실용적', 'ㅜㅜ'], ['한상기', '400d', '이젝큐티브', '선택', '이번', '구입', '10년', '이상', '생각', '시간', '디젤'], ['상기형', '리뷰'], ['만약', '포르쉐', '911', '근데', '파나메라', '진짜', '이쁘긴하다ㅠㅠ'], ['형님', '리뷰', '감사'], ['궁금', '20년식', '파나메라', '하이브리드', '기본인가요'], ['포르쉐'], ['1753', '1839', '한상기', '코너링'], ['뒷좌석', '터널', '공간', '저걸', '이유', '뭔가요'], ['운전석', '그날'], ['휴가', '때문', '리플', '수고'], ['포르쉐', '네오'], ['옵션', '2억', '이거', '15억', '설명', '옵션', '깡통', 'ㅠㅠ'], ['포르쉐', '형님', '볼게요'], ['정식', '수입', '직수', '차량', '한글화', '프로그램', '업데이트하', '파노라마'], ['클레스랑', '가격', '비슷', '추천하시'], ['이그제큐티브', '익제큐티브', '시승기'], ['8시리즈', '쿠페', '40i랑', '비교', '생각', '승차감', '운동성능이요'], ['0609', '로마', '주변', '한눈', '파노라마', '기능', '오타', '맞는듯싶네요'], ['포르쉐', '남자', '로망'], ['포르쉐', '신형', '아우디'], ['디자인', '현대차'], ['파나메', '시승', '구매', '이걸', '레인지로버', '보그', '이후', '극찬'], ['오우', '3시간'], ['기본', '옵션'], ['파나메라', '기존', '세그먼트', '세단', '클래스', '비교', '파나메라', '장점', '단점', '가지', '있을까요'], ['한국', '주차라인', '언제쯤'], ['아이폰', '처음', '기자님', '영상', '보네욯ㅎ'], ['파노라마'], ['롱휠베이스', '뒷자리', '승차감'], [], ['포르쉐', '파나메'], ['오오', '독국', '스팅어'], ['진짜', '정도', 'ㅡㅡ진짜'], ['14년식', '12', '파나메라', '중고차', '구매', '고장', '나긴', '정도', '가서요'], ['상기', '뮤ㅠ'], ['뭔가', '옛날', '모습', '발전', '무엇'], ['043', '매미', '리듬감', '뭔데'], ['파노라마'], [], ['장거리', '주행', '파나메', '스포츠', '투리스모', '아우', 'RS6', '가요'], ['파노라마', '1억', '5천'], ['상기', '뮤ㅠ'], ['번창하세요'], ['2억', '1억', '9천500'], ['2열', '리클라이닝'], ['근데', '가격', '시승', '차량', '1억', '8천', '1억', '5천', '같은데요'], ['파나메라', '10주년', '보군요'], ['고급', '세단', '카이엔보단', '조용해야조ㅡㅡ'], ['오늘', '계약금', '이그제큐티브'], ['330마력', '15억'], ['기본', '뒷좌석', '매력'], ['후방', '카메라', '해상', '실화'], ['1627', '볼트'], ['정도'], ['전기차', '시대', '성능', '디자인과', '브랜드'], ['형님', '뒷자리', '장시간', '답답', '궁금'], ['가격', '진짜'], ['포르쉐', '할인'], ['포르쉐', '파나메라', '시승기'], ['시승차량', '현금', '구매', '방법', '없겠죠ㅠ'], [], ['올림픽', '폐막식', '감사'], ['1220', '눈앞', '날파리'], ['지금', '조선', '파나메라', '대리만족', 'ㅜㅜ'], ['세단용', '포르쉐'], ['포르쉐', '실내', '앰비언트', '무드', '포르쉐', '안사', '마티즈', '이유'], [], ['오늘'], ['제로백', '52초', '아반떼'], ['718'], [], ['파나메라야', '미안'], ['선댓후감'], ['후방카메라'], ['파노라마'], ['검수', '하나'], ['1억', '오천', '벤츠', '가격', '생각'], ['이번'], ['옵션'], ['1억', '5천'], [], ['포르쉐', '점심'], ['1억', 'ㅎㅎ'], ['파나메라'], ['이걸'], ['돈값']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sum(youtube_review_han_nouns, [])\n",
    "count = Counter(word_list)\n",
    "word_count = dict(count.most_common())\n",
    "\n",
    "# 로컬에 있는 폰트를 사용할 경우, 폰트의 경로를 font_path에 추가 해주시면 됩니다.\n",
    "wc = WordCloud(font_path=fontpath, background_color = 'white',colormap=matplotlib.cm.inferno,  max_words=100, width=800, height=800, prefer_horizontal = True)\n",
    "cloud = wc.fit_words(word_count)\n",
    "cloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = car_df.drop(columns=['price','car_area','car_no', 'car_brand', 'car_name', 'name_datailed', 'fuel', 'car_type', 'color', 'trans', 'loss', 'flood', 'usage','insurance', 'sales_corp', 'sales_loca', 'options'])\n",
    "y = car_df[['price']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=True, normalize = True, copy_X = True)\n",
    "lr.fit(x_train, y_train)\n",
    "y_predict = lr.predict(x_test)\n",
    "print('LinearRegression trian 정확도 :', lr.score(x_train, y_train))\n",
    "print('LinearRegression test 정확도:', lr.score(x_test, y_test))\n",
    "print('LinearRegression test 정확도:', r2_score(y_test, lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(min_samples_leaf=10, min_samples_split=5, learning_rate=0.5, max_depth=3, n_estimators=1000)\n",
    "gb.fit(x_train, y_train)\n",
    "y_gb_predict = gb.predict(x_test)\n",
    "\n",
    "print('gb train 정확도:', gb.score(x_train, y_train))\n",
    "print('gb test 정확도:', gb.score(x_test, y_test))\n",
    "print('gb test 정확도:', r2_score(y_test, gb.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_gb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestRegressor(n_estimators = 50, random_state  = 42)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "pred = rf_clf.predict(x_test)\n",
    "\n",
    "print('RandomForest train 정확도:', rf_clf.score(x_train, y_train))\n",
    "print('RandomForest test 정확도:', rf_clf.score(x_test, y_test))\n",
    "print('RandomForest test 정확도:', r2_score(y_test, rf_clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "\n",
    "xgb_model.fit(x_train,y_train)\n",
    "predictions = xgb_model.predict(x_test)\n",
    "\n",
    "r_sq = xgb_model.score(x_train, y_train)\n",
    "print('XBoost train 정확도: ',r_sq)\n",
    "print('XBoost test 정확도: ',explained_variance_score(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
