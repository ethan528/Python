{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2253, 751)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 트레인 데이터랑 테스트 데이터 나누기\n",
    "train_data, test_data = train_test_split(df, test_size = 0.25, random_state = 42)\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 라벨링에 따른 분포도 막대그래프\n",
    "df_labeling['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  count\n",
       "0      0   2253"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨링에 따른 분포도 표\n",
    "train_data.groupby('label').size().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt; t = Okt()\n",
    "\n",
    "# Okt 실험\n",
    "t.morphs('정상에 선 기쁨을 다른 사람들과 함께 나누자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 설정\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레인 데이터 형태소 추출 및 불용어 제거\n",
    "train_data['tokenized'] = train_data['text'].apply(t.morphs)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 형태소 추출 및 불용어 제거\n",
    "test_data['tokenized'] = test_data['text'].apply(t.morphs)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 트레인 데이터에 라벨에 따른 형태소 분류 후 배열 만들기\n",
    "neutral_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "negative_words = np.hstack(train_data[train_data.label == -1]['tokenized'].values)\n",
    "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 빈도수가 높은 상위 20개 중립 단어\n",
    "neutral_word_count = Counter(neutral_words)\n",
    "print(neutral_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수가 높은 상위 20개 부정 단어\n",
    "negative_word_count = Counter(negative_words)\n",
    "print(negative_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수가 높은 상위 20개 긍정 단어\n",
    "positive_word_count = Counter(positive_words)\n",
    "print(positive_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 중립 wordcloud\n",
    "wc = WordCloud(font_path='C:/Windows/Fonts/malgun.ttf', width=400, height=400, scale=2.0, max_font_size=250)\n",
    "gen = wc.generate_from_frequencies(neutral_word_count)\n",
    "plt.figure()\n",
    "plt.imshow(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 부정 wordcloud\n",
    "wc = WordCloud(font_path='C:/Windows/Fonts/malgun.ttf', width=400, height=400, scale=2.0, max_font_size=250)\n",
    "gen = wc.generate_from_frequencies(negative_word_count)\n",
    "plt.figure()\n",
    "plt.imshow(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정 wordcloud\n",
    "wc = WordCloud(font_path='C:/Windows/Fonts/malgun.ttf', width=400, height=400, scale=2.0, max_font_size=250)\n",
    "gen = wc.generate_from_frequencies(positive_word_count)\n",
    "plt.figure()\n",
    "plt.imshow(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중립, 부정, 긍정에 따른 텍스트 길이 비교 시각화\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5)) # 크기 설정\n",
    "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x)) # 분류에 따른 텍스트 길이 추출\n",
    "ax1.hist(text_len, color='purple') # 히스토그램 그리기\n",
    "ax1.set_title('Neutral') # 제목 만들기\n",
    "ax1.set_xlabel('length of samples') # X축 이름\n",
    "ax1.set_ylabel('number of samples') # y축 이름\n",
    "print('중립의 평균 길이 :', np.mean(text_len)) # 평균 길이 출력\n",
    "\n",
    "text_len = train_data[train_data['label']==-1]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='blue')\n",
    "ax2.set_title('Negative')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('부정의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax3.hist(text_len, color='red')\n",
    "ax3.set_title('Positive')\n",
    "ax3.set_xlabel('length of samples')\n",
    "ax3.set_ylabel('number of samples')\n",
    "print('긍정의 평균 길이 :', np.mean(text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train) # 빈도수 기준으로 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 빈도수가 threshold보다 적은 단어의 총 합\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# key는 단어, value는 빈도수\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 빈도수가 threshold보다 작을 경우\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 2 # 0 = 패딩, 1 = OOV = +2\n",
    "vocab_size # 단어 집합의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 시퀀스로 변환\n",
    "tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train) # 앞에 만든 토큰의 인덱스만 추출해 배열 생성\n",
    "X_test = tokenizer.texts_to_sequences(X_test) # 앞에 만든 토큰의 인덱스만 추출해 배열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('최대 길이 :',max(len(text) for text in X_train))\n",
    "print('평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(text) for text in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "  print('전체 텍스트 중 길이가 %s 이하인 텍스트의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 3 # 값을 바꾸면서 비율 높이기\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 제로 패딩\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GRU(hidden_units))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 검증 데이터 손실이 4회 이상 조기 종료\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 검증 데이터의 정확도가 이전보다 상승했을 경우에만 저장\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2) # 20%의 검증 데이터를 분리해 훈련이 적절히 되는지 확인 및 과적합 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 저장된 모델을 불러와 정확도 테스트\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = t.morphs(new_sentence)\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len)\n",
    "    \n",
    "    score = float(loaded_model.predict(pad_new))\n",
    "    if score == 0:\n",
    "        print(\"{:.2f}% 확률로 중립입니다.\".format(score * 100))\n",
    "    elif score == -1 :\n",
    "        print(\"{:.2f}% 확률로 부정입니다.\".format((1 - score) * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 긍정입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
